{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b0749b",
   "metadata": {},
   "source": [
    "## CV â†’ TruCV AI Parsing Prototype (Laboratory)\n",
    "\n",
    "### Input: Resume (PDF / DOCX)\n",
    "\n",
    "- Output: TruCV Draft JSON + confidence + warnings\n",
    "\n",
    "- Uses: OpenAI API\n",
    "\n",
    "### Does NOT handle:\n",
    "\n",
    "- Verification\n",
    "\n",
    "- Blockchain\n",
    "\n",
    "- Database writes\n",
    "\n",
    "- Auth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b8e5d",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ CELL 1 â€” Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69aaf8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Core Python\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from typing import TypedDict, Literal, Dict, Any, List, Optional, Annotated\n",
    "import operator\n",
    "\n",
    "# ---- Environment ----\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---- File Parsing ----\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "\n",
    "# ---- LangGraph / LangChain ----\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# ---- Schema / Validation ----\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "#load env \n",
    "load_dotenv()\n",
    "\n",
    "#selecting models\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "#global veriable \n",
    "CONFIDENCE_THRESHOLD = 0.65\n",
    "MAX_RETRIES_PER_SECTION = 1\n",
    "\n",
    "SUPPORTED_SECTIONS = [\n",
    "    \"personal\",\n",
    "    \"education\",\n",
    "    \"experience\",\n",
    "    \"skills\",\n",
    "    \"projects\",\n",
    "    \"awards\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6953a94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cec719",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 2 â€” TruCV Target Schema (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c050cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 . Common / Reusable Models\n",
    "\n",
    "class TruCVBaseModel(BaseModel):\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"ignore\"\n",
    "\n",
    "# from is a Python keyword â†’ use from_ + alias=\"from\"\n",
    "class Duration(TruCVBaseModel):\n",
    "    from_: Optional[str] = Field(None, alias=\"from\")\n",
    "    to: Optional[str] = None\n",
    "\n",
    "#1 . Personal Section\n",
    "class PersonalInfo(TruCVBaseModel):\n",
    "    fullName: Optional[str] = None\n",
    "    email: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "    city: Optional[str] = None\n",
    "    linkedin: Optional[str] = None\n",
    "    github: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    imgUrl: Optional[str] = None\n",
    "\n",
    "#2. Education Model\n",
    "class Education(TruCVBaseModel):\n",
    "    id: Optional[str] = None\n",
    "    # ðŸ› ï¸ ADDED: Missing from Python but present in TS Interface\n",
    "    eduDocId: Optional[str] = None \n",
    "    level: Optional[str] = None\n",
    "    boardNameOrDegree: Optional[str] = None\n",
    "    institutionName: Optional[str] = None\n",
    "    gpa: Optional[str] = None\n",
    "    duration: Optional[Duration] = None\n",
    "\n",
    "    selfAttested: bool = True\n",
    "    docUri: Optional[str] = None\n",
    "    issuerEmailId: Optional[str] = \"\"\n",
    "    isEmailSend: bool = False\n",
    "\n",
    "    verified: bool = False\n",
    "    # ðŸ› ï¸ FIXED: Removed \"rejected\" to match Mongoose enum [\"pending\", \"verified\"]\n",
    "    status: Literal[\"pending\", \"verified\"] = \"pending\"\n",
    "\n",
    "\n",
    "#3. Experience Model\n",
    "class Experience(TruCVBaseModel):\n",
    "    id: Optional[str] = None\n",
    "    companyName: Optional[str] = None\n",
    "    jobRole: Optional[str] = None\n",
    "    duration: Optional[Duration] = None\n",
    "    skills: Optional[str] = None # Note: AI must format this as comma-separated string\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    selfAttested: bool = True\n",
    "    isEmailSend: bool = False\n",
    "    docUri: Optional[str] = None\n",
    "    issuerEmailId: Optional[str] = \"\"\n",
    "\n",
    "    verified: bool = False\n",
    "    # ðŸ› ï¸ FIXED: Removed \"rejected\"\n",
    "    status: Literal[\"pending\", \"verified\"] = \"pending\"\n",
    "\n",
    "#4. Skills Model\n",
    "class Skill(TruCVBaseModel):\n",
    "    id: Optional[str] = None\n",
    "    skillName: Optional[str] = None\n",
    "    level: Optional[str] = None\n",
    "\n",
    "    selfAttested: bool = True\n",
    "    # âš ï¸ NOTE: Typo 'endoresBy' matches DB schema. DO NOT CORRECT.\n",
    "    endoresBy: Optional[str] = \"\"\n",
    "    endoresThrough: Optional[str] = \"\"\n",
    "\n",
    "#5. Project Model\n",
    "class Project(TruCVBaseModel):\n",
    "    id: Optional[str] = None\n",
    "    projectName: Optional[str] = None\n",
    "    projectUrl: Optional[str] = None\n",
    "    duration: Optional[Duration] = None\n",
    "    skills: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    selfAttested: bool = True\n",
    "\n",
    "#6. Award / Certificate Model\n",
    "class Award(TruCVBaseModel):\n",
    "    id: Optional[str] = None\n",
    "    level: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    organisation: Optional[str] = None\n",
    "    duration: Optional[Duration] = None\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    selfAttested: bool = True\n",
    "    issuerEmailId: Optional[str] = \"\"\n",
    "    docUri: Optional[str] = \"\"\n",
    "    isEmailSend: bool = False\n",
    "\n",
    "    verified: bool = False\n",
    "    # ðŸ› ï¸ FIXED: Removed \"rejected\"\n",
    "    status: Literal[\"pending\", \"verified\"] = \"pending\"\n",
    "\n",
    "# Final TruCV Root Model\n",
    "class TruCVDraft(TruCVBaseModel):\n",
    "    userId: Optional[str] = None\n",
    "    title: Optional[str] = None\n",
    "\n",
    "    personal: PersonalInfo\n",
    "\n",
    "    educations: List[Education] = Field(default_factory=list)\n",
    "    experiences: List[Experience] = Field(default_factory=list)\n",
    "    skills: List[Skill] = Field(default_factory=list)\n",
    "    projects: List[Project] = Field(default_factory=list)\n",
    "    awards: List[Award] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "674fc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userId': 'test_user', 'title': 'Auto Generated CV', 'personal': {'fullName': 'Test User', 'email': 'test@example.com', 'phone': None, 'city': None, 'linkedin': None, 'github': None, 'summary': None, 'imgUrl': None}, 'educations': [{'id': None, 'eduDocId': None, 'level': 'Graduation', 'boardNameOrDegree': None, 'institutionName': 'Test University', 'gpa': None, 'duration': {'from': '2020', 'to': '2024'}, 'selfAttested': True, 'docUri': None, 'issuerEmailId': '', 'isEmailSend': False, 'verified': False, 'status': 'pending'}], 'experiences': [], 'skills': [], 'projects': [], 'awards': []}\n"
     ]
    }
   ],
   "source": [
    "test_data = {\n",
    "    \"userId\": \"test_user\",\n",
    "    \"title\": \"Auto Generated CV\",\n",
    "    \"personal\": {\n",
    "        \"fullName\": \"Test User\",\n",
    "        \"email\": \"test@example.com\"\n",
    "    },\n",
    "    \"educations\": [\n",
    "        {\n",
    "            \"level\": \"Graduation\",\n",
    "            \"institutionName\": \"Test University\",\n",
    "            \"duration\": {\n",
    "                \"from\": \"2020\",\n",
    "                \"to\": \"2024\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "cv = TruCVDraft(**test_data)\n",
    "print(cv.model_dump(by_alias=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a98d6",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 3 â€” Internal AI Parsing Schema (Intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0cb5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsedBaseModel(BaseModel):\n",
    "    class Config:\n",
    "        extra = \"ignore\"\n",
    " \n",
    "# 1ï¸. Atomic Parsed Field\n",
    "class ParsedField(ParsedBaseModel):\n",
    "    value: Optional[str] = None\n",
    "    confidence: float = Field(\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )\n",
    "#2. Parsed Personal Section\n",
    "class ParsedPersonal(ParsedBaseModel):\n",
    "    fullName: Optional[ParsedField] = None\n",
    "    email: Optional[ParsedField] = None\n",
    "    phone: Optional[ParsedField] = None\n",
    "    city: Optional[ParsedField] = None\n",
    "    linkedin: Optional[ParsedField] = None\n",
    "    github: Optional[ParsedField] = None\n",
    "    summary: Optional[ParsedField] = None\n",
    "\n",
    "\n",
    "#3.Parsed Education\n",
    "class ParsedEducationEntry(ParsedBaseModel):\n",
    "    level: Optional[ParsedField] = None\n",
    "    boardNameOrDegree: Optional[ParsedField] = None\n",
    "    institutionName: Optional[ParsedField] = None\n",
    "    gpa: Optional[ParsedField] = None\n",
    "    duration_from: Optional[ParsedField] = None\n",
    "    duration_to: Optional[ParsedField] = None\n",
    "\n",
    "class ParsedEducation(ParsedBaseModel):\n",
    "    items: List[ParsedEducationEntry] = Field(default_factory=list)\n",
    "    section_confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "#4. Parsed Experience\n",
    "class ParsedExperienceEntry(ParsedBaseModel):\n",
    "    companyName: Optional[ParsedField] = None\n",
    "    jobRole: Optional[ParsedField] = None\n",
    "    skills: Optional[ParsedField] = None\n",
    "    description: Optional[ParsedField] = None\n",
    "    duration_from: Optional[ParsedField] = None\n",
    "    duration_to: Optional[ParsedField] = None\n",
    "\n",
    "class ParsedExperience(ParsedBaseModel):\n",
    "    items: List[ParsedExperienceEntry] = Field(default_factory=list)\n",
    "    section_confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "#5.Parsed Skills \n",
    "class ParsedSkillEntry(ParsedBaseModel):\n",
    "    skillName: Optional[ParsedField] = None\n",
    "    level: Optional[ParsedField] = None\n",
    "\n",
    "class ParsedSkills(ParsedBaseModel):\n",
    "    items: List[ParsedSkillEntry] = Field(default_factory=list)\n",
    "    section_confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "#6. Parsed Projects\n",
    "class ParsedProjectEntry(ParsedBaseModel):\n",
    "    projectName: Optional[ParsedField] = None\n",
    "    projectUrl: Optional[ParsedField] = None\n",
    "    skills: Optional[ParsedField] = None\n",
    "    description: Optional[ParsedField] = None\n",
    "    duration_from: Optional[ParsedField] = None\n",
    "    duration_to: Optional[ParsedField] = None\n",
    "\n",
    "class ParsedProjects(ParsedBaseModel):\n",
    "    items: List[ParsedProjectEntry] = Field(default_factory=list)\n",
    "    section_confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "#7. Parsed Awards\n",
    "class ParsedAwardEntry(ParsedBaseModel):\n",
    "    level: Optional[ParsedField] = None\n",
    "    name: Optional[ParsedField] = None\n",
    "    organisation: Optional[ParsedField] = None\n",
    "    description: Optional[ParsedField] = None\n",
    "    duration_from: Optional[ParsedField] = None\n",
    "    duration_to: Optional[ParsedField] = None\n",
    "\n",
    "class ParsedAwards(ParsedBaseModel):\n",
    "    items: List[ParsedAwardEntry] = Field(default_factory=list)\n",
    "    section_confidence: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "#8. Parsed Resume\n",
    "class ParsedResume(ParsedBaseModel):\n",
    "    personal: Optional[ParsedPersonal] = None\n",
    "    education: Optional[ParsedEducation] = None\n",
    "    experience: Optional[ParsedExperience] = None\n",
    "    skills: Optional[ParsedSkills] = None\n",
    "    projects: Optional[ParsedProjects] = None\n",
    "    awards: Optional[ParsedAwards] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a11483",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 4 â€” Resume Upload (PDF / DOCX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0013ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume file loaded successfully: /home/ganesh/Desktop/LanChain-Framework/Ganesh-Agrahari-Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "uploaded_file_path = \"/home/ganesh/Desktop/LanChain-Framework/Ganesh-Agrahari-Resume.pdf\" \n",
    "\n",
    "if not os.path.exists(uploaded_file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {uploaded_file_path}\")\n",
    "\n",
    "file_ext = os.path.splitext(uploaded_file_path)[1].lower()\n",
    "\n",
    "if file_ext not in [\".pdf\", \".docx\"]:\n",
    "    raise ValueError(\"Unsupported file type. Only PDF and DOCX are allowed.\")\n",
    "\n",
    "print(f\"Resume file loaded successfully: {uploaded_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729f90a",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 5 â€” Text Extraction Layer (PDF / DOCX)\n",
    "\n",
    "- Inputs\n",
    " uploaded_file_path\n",
    "\n",
    "- Outputs\n",
    "raw_text\n",
    "normalized_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcb5509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘€ Starting Vision Extraction...\n",
      "   Processing Page 1/2...\n",
      "   Processing Page 2/2...\n",
      "âœ… Extraction Complete! Length: 5789 chars\n",
      "--- PREVIEW (First 500 chars) ---\n",
      "```\n",
      "Ganesh Agrahari\n",
      "\n",
      "ganeshagrahari08@gmail.com\n",
      "+91 9044232872\n",
      "Lucknow, India\n",
      "My Portfolio\n",
      "LinkedIn\n",
      "GitHub\n",
      "\n",
      "PROFILE\n",
      "AI Engineer with hands-on experience building real-world, production-grade systems. Developed serverless AI microservices, Retrieval-Augmented Generation (RAG) systems using embedding models, and scalable vector search pipelines. Experienced with Azure Functions, GPT-4, Elasticsearch, LangGraph, and N8n-based automation workflows. Strong foundation in Python, Machine Learning, NLP,\n"
     ]
    }
   ],
   "source": [
    "# ðŸŸ¦ CELL 5 â€” Visual Text Extraction (The \"Human Eye\" Approach)\n",
    "\n",
    "import base64\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def encode_image_base64(image):\n",
    "    \"\"\"Convert PIL Image to base64 string for OpenAI\"\"\"\n",
    "    import io\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "def extract_text_via_vision(file_path: str, file_ext: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT-4o Vision to read the resume layout visually.\n",
    "    This solves multi-column overlap issues perfectly.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    \n",
    "    print(\"ðŸ‘€ Starting Vision Extraction...\")\n",
    "\n",
    "    if file_ext == \".pdf\":\n",
    "        # 1. Convert PDF pages to Images\n",
    "        images = convert_from_path(file_path)\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            print(f\"   Processing Page {i+1}/{len(images)}...\")\n",
    "            base64_img = encode_image_base64(img)\n",
    "            \n",
    "            # 2. Ask GPT-4o to transcribe the image\n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\", \n",
    "                        \"text\": \"You are a professional resume parser. Transcribe the text from this resume page exactly as it appears, but strictly preserving the logical reading order. If there are multiple columns, read the left column completely, then the right column. Do not mix text from different columns. Output only the raw text.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            response = llm.invoke([message])\n",
    "            full_text += response.content + \"\\n\\n\"\n",
    "            \n",
    "    elif file_ext == \".docx\":\n",
    "        # DOCX is already linear, no need for vision\n",
    "        doc = Document(file_path)\n",
    "        full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# ---- EXECUTION ----\n",
    "try:\n",
    "    # This replaces the old pdfplumber logic entirely\n",
    "    raw_text = extract_text_via_vision(uploaded_file_path, file_ext)\n",
    "    \n",
    "    if not raw_text.strip():\n",
    "        raise ValueError(\"Vision extraction returned empty text.\")\n",
    "\n",
    "    # We still normalize to clean up any AI artifacts\n",
    "    def normalize_text(text: str) -> str:\n",
    "        text = re.sub(r\"[â€¢â—¦â–ªâ—â˜…ï‚§]\", \"-\", text) # Bullets\n",
    "        text = re.sub(r\" +\", \" \", text)       # Extra spaces\n",
    "        text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text) # Extra newlines\n",
    "        return text.strip()\n",
    "\n",
    "    normalized_text = normalize_text(raw_text)\n",
    "\n",
    "    print(f\"âœ… Extraction Complete! Length: {len(normalized_text)} chars\")\n",
    "    print(\"--- PREVIEW (First 500 chars) ---\")\n",
    "    print(normalized_text[:500])\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Vision Extraction Failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e75f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "Ganesh Agrahari\n",
      "\n",
      "ganeshagrahari08@gmail.com\n",
      "+91 9044232872\n",
      "Lucknow, India\n",
      "My Portfolio\n",
      "LinkedIn\n",
      "GitHub\n",
      "\n",
      "PROFILE\n",
      "AI Engineer with hands-on experience building real-world, production-grade systems. Developed serverless AI microservices, Retrieval-Augmented Generation (RAG) systems using embedding models, and scalable vector search pipelines. Experienced with Azure Functions, GPT-4, Elasticsearch, LangGraph, and N8n-based automation workflows. Strong foundation in Python, Machine Learning, NLP, and cloud architecture, with proven experience self-hosting Elasticsearch and N8n on Azure Virtual Machines to deliver fast, reliable, and cost-efficient AI solutions at scale.\n",
      "\n",
      "EDUCATION\n",
      "BCA Data Science & Artificial Intelligence(in collaboration with IBM)\n",
      "BBD University\n",
      "08/2023 â€“ 09/2026 | Lucknow, India\n",
      "Last year SGPA: 8\n",
      "\n",
      "Intermediate(PCM)\n",
      "SVM Inter College Ntpc\n",
      "2022 | Raebareli, India\n",
      "Percentage: 82%\n",
      "\n",
      "ACTIVITY\n",
      "HackerRun Problem Solving\n",
      "03/2025 â€“ present\n",
      "\n",
      "GitHub Streak Maintenance\n",
      "10/2024 â€“ present\n",
      "\n",
      "Member of GDG(Google's Developer Group)\n",
      "Lucknow, India\n",
      "\n",
      "Member of Technical team\n",
      "BBD University, Lucknow, India\n",
      "\n",
      "LeetCode Problem Solving\n",
      "\n",
      "AI/ML Workshops Attendee\n",
      "Lucknow, India\n",
      "\n",
      "PROFESSIONAL EXPERIENCE\n",
      "AI Engineer Intern\n",
      "Edubuk\n",
      "08/2025 â€“ Present\n",
      "- Built an AI-powered JDâ€“CV matching system combining vector similarity for accuracy and LLM-based analysis for contextual understanding, enabling high-precision candidateâ€“job matching at scale.\n",
      "- Designed and deployed a serverless architecture on Azure using Azure Functions, including migration from an earlier AWS Lambdaâ€“based serverless setup, ensuring improved scalability and operational consistency.\n",
      "- Implemented a Retrieval-Augmented pipeline using OpenAI text-embedding-3-large for semantic embeddings and GPT-4 for deep context evaluation between job descriptions and resumes.\n",
      "- Migrated the search infrastructure from AWS OpenSearch to self-hosted Elasticsearch on Azure Virtual Machines, improving system control, flexibility, and long-term cost efficiency.\n",
      "- Engineered backend services for resume ingestion, JD processing, and matching orchestration, delivering fast, reliable responses suitable for production-grade hiring workflows.\n",
      "\n",
      "AI/GenAi Intern at QTechSolutions\n",
      "AICTE\n",
      "03/2023 â€“ 08/2025\n",
      "- Working on a real-world healthcare site to develop and integrate an AI-powered chatbot for doctor consultations, medicine delivery, and prescription recommendations.\n",
      "- Implementing NLP and machine learning techniques to enhance chatbot accuracy, ensuring seamless and efficient user interactions in a scalable AI-driven system.\n",
      "- Leveraging Retrieval-Augmented Generation (RAG) to provide dynamic, context-aware responses by combining LLM capabilities with real-time retrieval from healthcare databases.\n",
      "\n",
      "Data Science Intern\n",
      "Unified Mentor\n",
      "11/2024 â€“ 01/2025\n",
      "- Implemented a text classification pipeline, improving document categorization accuracy by 20%\n",
      "- Developed an NLP-based climate change analysis system using sentiment analysis and topic modeling to extract insights from global news articles.\n",
      "\n",
      "PROJECTS\n",
      "TruJobs â€“ AI Recruitment System (Edubuk)\n",
      "- Designed an AI-driven JDâ€“CV matching workflow that evaluates both semantic similarity and contextual relevance, improving hiring signal quality beyond keyword-based matching.\n",
      "- Implemented an embedding-based retrieval layer using OpenAI text-embedding-3-large, enabling accurate semantic comparison between resumes and job descriptions.\n",
      "- Integrated GPT-4 to perform contextual analysis and reasoning over retrieved candidates, enhancing match quality for complex and non-obvious skill relations.\n",
      "- Built event-driven APIs for resume ingestion, JD uploads, and match retrieval, optimized for high-throughput processing in a production environment.\n",
      "- Enabled real-time schema flexibility and fast search operations using self-hosted Elasticsearch, supporting continuous data evolution without service disruption.\n",
      "```\n",
      "\n",
      "**CERTIFICATES**\n",
      "\n",
      "- Data Science Level 1 - IBM\n",
      "- Analytics in IBM Cognos\n",
      "- Machine Learning - Udemy\n",
      "- Cyber Security - Microsoft\n",
      "- App Development - BBDU\n",
      "- NoSql & DbaaS 101 - IBM\n",
      "- Data Science 101 - IBM\n",
      "- Predictive Modeling Fundamentals I - IBM\n",
      "- Python 101 for Data Science - IBM\n",
      "\n",
      "Face Recognition Attendance System:\n",
      "- Designed and deployed a real-time face recognition attendance system using Python, OpenCV, and CNN, achieving 95%+ accuracy and reducing manual attendance tracking efforts by over 40%. Integrated automated data logging and Excel export for seamless record-keeping.\n",
      "\n",
      "Next.js Portfolio site:\n",
      "- Interactive Portfolio Website - Built using Next.js and deployed on Vercel, featuring a responsive design and an AI-powered chatbot for seamless user interaction. Showcases my skills, projects, and experiences with a dynamic and engaging UI.\n",
      "\n",
      "**SKILLS**\n",
      "\n",
      "**Programming & Scripting:**\n",
      "- Python, JavaScript\n",
      "- Async Programming, REST API Development\n",
      "\n",
      "**AI/ML & LLM Engineering:**\n",
      "- Machine Learning, Deep Learning, NLP, LLMs, RAG\n",
      "- Vector Search (Elasticsearch), Multi-Vector Embeddings (3072-dim)\n",
      "- Hybrid Scoring (GPT-4 + vector similarity), Prompt Engineering\n",
      "- Frameworks: PyTorch, LangChain, LangGraph Scikit-learn, OpenCV\n",
      "- Automation Tools: N8N\n",
      "\n",
      "**Cloud & Backend Architecture:**\n",
      "- Azure (current): Functions, IaaS(VMs), API Management, Blob Storage, App Insights, Azure OpenAI (GPT-4/Embeddings)\n",
      "- AWS (previous version of TruJobs): Lambda, Amazon Bedrock (Claude 3 Haiku, Titan Embeddings), API Gateway, S3, OpenSearch\n",
      "- Microservices, Serverless Architecture, Async Pipelines\n",
      "- CI/CD, Git, GitHub\n",
      "\n",
      "**Data Engineering & Analytics:**\n",
      "- Data Processing, ETL, Feature Engineering\n",
      "- Power BI, Jupyter Notebook, Matplotlib, Seaborn\n",
      "\n",
      "**Databases & Storage:**\n",
      "- SQL\n",
      "- Elasticsearch (vector + keyword fields)\n",
      "- AWS S3, Azure Blob Storage\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf2fad",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 6 â€” Section Detection (Correct Way)\n",
    "### Inputs\n",
    "- normalized_text\n",
    "### Outputs\n",
    "- sections = {\n",
    "    \"personal\": str,\n",
    "    \"education\": str,\n",
    "    \"experience\": str,\n",
    "    \"skills\": str,\n",
    "    \"projects\": str,\n",
    "    \"awards\": str\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eed0b7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Detecting sections semantically (No Regex)...\n",
      "\n",
      "âœ… Sections Detected Successfully!\n",
      "   ðŸ“‚ PERSONAL     : 102  chars | Ganesh Agrahari  ganeshagrahari08@gmail.com +91 9044232872 Lucknow, India My Portfolio LinkedIn GitH...\n",
      "   ðŸ“‚ EDUCATION    : 219  chars | BCA Data Science & Artificial Intelligence(in collaboration with IBM) BBD University 08/2023 â€“ 09/20...\n",
      "   ðŸ“‚ EXPERIENCE   : 1855 chars | AI Engineer Intern Edubuk 08/2025 â€“ Present - Built an AI-powered JDâ€“CV matching system combining ve...\n",
      "   ðŸ“‚ SKILLS       : 980  chars | **Programming & Scripting:** - Python, JavaScript - Async Programming, REST API Development  **AI/ML...\n",
      "   ðŸ“‚ PROJECTS     : 1427 chars | TruJobs â€“ AI Recruitment System (Edubuk) - Designed an AI-driven JDâ€“CV matching workflow that evalua...\n",
      "   ðŸ“‚ AWARDS       : 265  chars | - Data Science Level 1 - IBM - Analytics in IBM Cognos - Machine Learning - Udemy - Cyber Security -...\n"
     ]
    }
   ],
   "source": [
    "# ðŸŸ¦ CELL 6 â€” Semantic Section Detection (The \"Router\")\n",
    "\n",
    "# 1. Define the Router Schema (Strict Contract)\n",
    "class SectionSplit(BaseModel):\n",
    "    personal: str = Field(description=\"Raw text containing name, contact, links, summary, and profile image url\")\n",
    "    education: str = Field(description=\"Raw text containing degrees, universities, dates, and grades\")\n",
    "    experience: str = Field(description=\"Raw text containing job roles, companies, dates, and responsibilities\")\n",
    "    skills: str = Field(description=\"Raw text containing technical skills, soft skills, tools, and languages\")\n",
    "    projects: str = Field(description=\"Raw text containing project names, descriptions, and links\")\n",
    "    awards: str = Field(description=\"Raw text containing certifications, honors, and achievements\")\n",
    "\n",
    "def detect_sections_semantically(text: str) -> Dict[str, str]:\n",
    "    print(\"ðŸ§  Detecting sections semantically (No Regex)...\")\n",
    "    \n",
    "    # 2. System Prompt: The \"Router\" instructions\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert Resume Segmenter. \n",
    "    Your task is to classify every single line of the provided resume text into one of these 6 buckets:\n",
    "    - personal\n",
    "    - education\n",
    "    - experience\n",
    "    - skills\n",
    "    - projects\n",
    "    - awards\n",
    "\n",
    "    CRITICAL RULES:\n",
    "    1. Do not summarize. Copy the EXACT text from the resume into the correct bucket.\n",
    "    2. If a section is missing (e.g. no Awards), return an empty string for that key.\n",
    "    3. If text is ambiguous (e.g. \"React\" inside a project description), keep it with the project.\n",
    "    4. Personal section MUST include the Name, Phone, Email, and Links found at the top.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Use 'with_structured_output' to prevent JSON crashes\n",
    "    # This forces the LLM to call a function, returning a valid Pydantic object\n",
    "    router_llm = llm.with_structured_output(SectionSplit)\n",
    "    \n",
    "    try:\n",
    "        response = router_llm.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=f\"RESUME TEXT:\\n{text}\")\n",
    "        ])\n",
    "        \n",
    "        # Convert Pydantic model to Python Dict\n",
    "        sections = response.model_dump()\n",
    "        return sections\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Semantic Section Detection Failed: {str(e)}\")\n",
    "\n",
    "# ---- EXECUTION ----\n",
    "try:\n",
    "    # No more 'USE_LLM_FALLBACK' logic. We trust the Intelligence Layer.\n",
    "    resume_sections = detect_sections_semantically(normalized_text)\n",
    "\n",
    "    # ---- VISUAL SANITY CHECK ----\n",
    "    print(f\"\\nâœ… Sections Detected Successfully!\")\n",
    "    for k, v in resume_sections.items():\n",
    "        # Print first 100 chars of each section to verify content\n",
    "        preview = v[:100].replace('\\n', ' ') + \"...\" if len(v) > 0 else \"EMPTY\"\n",
    "        print(f\"   ðŸ“‚ {k.upper():<12} : {len(v):<4} chars | {preview}\")\n",
    "\n",
    "    # Safety Check for key sections\n",
    "    if len(resume_sections['experience']) < 20 and len(resume_sections['education']) < 20:\n",
    "        print(\"\\nâš ï¸  WARNING: Critical sections (Experience/Education) seem empty.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ FATAL ERROR in Cell 6: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d482d",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 7 â€” Section Parsing (AI-Powered)\n",
    "\n",
    "### Inputs\n",
    "- sections\n",
    "### Outputs\n",
    "- parsed_sections = {\n",
    "    \"personal\": {...},\n",
    "    \"education\": [...],\n",
    "    \"experience\": [...],\n",
    "    \"skills\": [...],\n",
    "    \"projects\": [...],\n",
    "    \"awards\": [...]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a359d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¦ CELL 7 â€” Prompt Templates (Read-Only Configuration)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# GLOBAL INSTRUCTION (Appended to all prompts)\n",
    "# ------------------------------------------------------------------\n",
    "BASE_INSTRUCTION = \"\"\"\n",
    "You are a strict data extraction AI.\n",
    "INPUT: Raw text from a specific section of a resume.\n",
    "OUTPUT: Valid JSON matching the exact schema requested.\n",
    "\n",
    "RULES:\n",
    "1. DATA TYPES: Return every field as an object with two keys:\n",
    "   - \"value\": The extracted string (or null if not found).\n",
    "   - \"confidence\": A float (0.0 to 1.0) indicating certainty.\n",
    "   \n",
    "2. DATES: Format all dates as \"YYYY-MM\" (e.g., \"2023-08\"). \n",
    "   - If currently active, use \"Present\".\n",
    "   - If only year is available, use \"YYYY\".\n",
    "   \n",
    "3. MISSING DATA: \n",
    "   - If a field is not found, set \"value\": null and \"confidence\": 0.0.\n",
    "   - DO NOT hallucinate or invent data.\n",
    "\n",
    "4. NO MARKDOWN: Return ONLY the raw JSON string. No ```json blocks.\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. PERSONAL SECTION\n",
    "# ------------------------------------------------------------------\n",
    "PROMPT_PERSONAL = f\"\"\"\n",
    "{BASE_INSTRUCTION}\n",
    "\n",
    "EXTRACT THESE FIELDS:\n",
    "- fullName: The candidate's full name.\n",
    "- email: Valid email address.\n",
    "- phone: Phone number (normalize to standard format).\n",
    "- city: City/Location (e.g. \"Lucknow, India\").\n",
    "- linkedin: Full LinkedIn URL.\n",
    "- github: Full GitHub URL.\n",
    "- summary: A brief professional summary (max 3-4 lines).\n",
    "\n",
    "SCHEMA TARGET:\n",
    "{{\n",
    "  \"fullName\": {{ \"value\": \"...\", \"confidence\": 1.0 }},\n",
    "  \"email\": {{ \"value\": \"...\", \"confidence\": 1.0 }},\n",
    "  \"phone\": {{ \"value\": \"...\", \"confidence\": 0.8 }},\n",
    "  \"city\": {{ \"value\": \"...\", \"confidence\": 0.9 }},\n",
    "  \"linkedin\": {{ \"value\": \"...\", \"confidence\": 0.95 }},\n",
    "  \"github\": {{ \"value\": \"...\", \"confidence\": 0.95 }},\n",
    "  \"summary\": {{ \"value\": \"...\", \"confidence\": 0.8 }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. EDUCATION SECTION\n",
    "# ------------------------------------------------------------------\n",
    "PROMPT_EDUCATION = f\"\"\"\n",
    "{BASE_INSTRUCTION}\n",
    "\n",
    "EXTRACT A LIST OF EDUCATION ENTRIES.\n",
    "For each entry, extract:\n",
    "- level: \"Grade 10\", \"Grade 12\", \"Undergraduate\", \"Postgraduate\", or \"PhD\".\n",
    "- boardNameOrDegree: Degree name (e.g. \"B.Tech Computer Science\", \"CBSE\").\n",
    "- institutionName: University or School name.\n",
    "- gpa: CGPA or Percentage (as a string, e.g. \"8.5/10\" or \"82%\").\n",
    "- duration_from: Start date.\n",
    "- duration_to: End date (or \"Present\").\n",
    "\n",
    "SCHEMA TARGET:\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"level\": {{ \"value\": \"...\", \"confidence\": 0.9 }},\n",
    "      \"boardNameOrDegree\": {{ \"value\": \"...\", \"confidence\": 0.9 }},\n",
    "      \"institutionName\": {{ \"value\": \"...\", \"confidence\": 0.95 }},\n",
    "      \"gpa\": {{ \"value\": \"...\", \"confidence\": 0.8 }},\n",
    "      \"duration_from\": {{ \"value\": \"2020-08\", \"confidence\": 0.9 }},\n",
    "      \"duration_to\": {{ \"value\": \"2024-06\", \"confidence\": 0.9 }}\n",
    "    }}\n",
    "  ],\n",
    "  \"section_confidence\": 0.95\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. EXPERIENCE SECTION\n",
    "# ------------------------------------------------------------------\n",
    "PROMPT_EXPERIENCE = f\"\"\"\n",
    "{BASE_INSTRUCTION}\n",
    "\n",
    "EXTRACT A LIST OF PROFESSIONAL EXPERIENCES.\n",
    "For each entry, extract:\n",
    "- companyName: Name of the company.\n",
    "- jobRole: Title (e.g. \"Senior Software Engineer\").\n",
    "- description: Bullet points describing responsibilities (preserve newlines).\n",
    "- skills: A comma-separated string of tools/skills mentioned IN THIS ROLE (e.g. \"Python, Azure, Docker\").\n",
    "- duration_from: Start date.\n",
    "- duration_to: End date (or \"Present\").\n",
    "\n",
    "SCHEMA TARGET:\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"companyName\": {{ \"value\": \"...\", \"confidence\": 0.95 }},\n",
    "      \"jobRole\": {{ \"value\": \"...\", \"confidence\": 0.95 }},\n",
    "      \"description\": {{ \"value\": \"...\", \"confidence\": 0.85 }},\n",
    "      \"skills\": {{ \"value\": \"Java, Spring Boot\", \"confidence\": 0.8 }},\n",
    "      \"duration_from\": {{ \"value\": \"...\", \"confidence\": 0.9 }},\n",
    "      \"duration_to\": {{ \"value\": \"...\", \"confidence\": 0.9 }}\n",
    "    }}\n",
    "  ],\n",
    "  \"section_confidence\": 0.9\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. SKILLS SECTION\n",
    "# ------------------------------------------------------------------\n",
    "PROMPT_SKILLS = f\"\"\"\n",
    "{BASE_INSTRUCTION}\n",
    "\n",
    "EXTRACT A LIST OF SKILLS.\n",
    "For each skill:\n",
    "- skillName: The specific skill (e.g. \"React.js\", \"Python\").\n",
    "- level: infer \"Beginner\", \"Intermediate\", or \"Expert\" based on context. Default to \"Intermediate\" if unsure.\n",
    "\n",
    "SCHEMA TARGET:\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"skillName\": {{ \"value\": \"Python\", \"confidence\": 1.0 }},\n",
    "      \"level\": {{ \"value\": \"Expert\", \"confidence\": 0.8 }}\n",
    "    }}\n",
    "  ],\n",
    "  \"section_confidence\": 0.9\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. PROJECTS SECTION\n",
    "# ------------------------------------------------------------------\n",
    "PROMPT_PROJECTS = f\"\"\"\n",
    "{BASE_INSTRUCTION}\n",
    "\n",
    "EXTRACT A LIST OF PROJECTS.\n",
    "For each entry:\n",
    "- projectName: Name of the project.\n",
    "- projectUrl: Link to code or demo (if available).\n",
    "- description: Brief description of what was built.\n",
    "- skills: Comma-separated list of tech stack used.\n",
    "- duration_from: Start date (optional).\n",
    "- duration_to: End date (optional).\n",
    "\n",
    "SCHEMA TARGET:\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"projectName\": {{ \"value\": \"...\", \"confidence\": 0.9 }},\n",
    "      \"projectUrl\": {{ \"value\": \"...\", \"confidence\": 0.95 }},\n",
    "      \"description\": {{ \"value\": \"...\", \"confidence\": 0.8 }},\n",
    "      \"skills\": {{ \"value\": \"...\", \"confidence\": 0.8 }},\n",
    "      \"duration_from\": {{ \"value\": null, \"confidence\": 0.0 }},\n",
    "      \"duration_to\": {{ \"value\": null, \"confidence\": 0.0 }}\n",
    "    }}\n",
    "  ],\n",
    "  \"section_confidence\": 0.9\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. AWARDS SECTION\n",
    "# ------------------------------------------------------------------\n",
    "PROMPT_AWARDS = f\"\"\"\n",
    "{BASE_INSTRUCTION}\n",
    "\n",
    "EXTRACT A LIST OF AWARDS/CERTIFICATIONS.\n",
    "For each entry:\n",
    "- name: Name of the award or certificate.\n",
    "- organisation: Issuing body (e.g. \"AWS\", \"Google\", \"University\").\n",
    "- level: \"National\", \"International\", \"University\", or \"Other\".\n",
    "- description: Any details.\n",
    "- duration_from: Date received.\n",
    "\n",
    "SCHEMA TARGET:\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"name\": {{ \"value\": \"AWS Certified Solutions Architect\", \"confidence\": 0.95 }},\n",
    "      \"organisation\": {{ \"value\": \"Amazon Web Services\", \"confidence\": 0.9 }},\n",
    "      \"level\": {{ \"value\": \"International\", \"confidence\": 0.7 }},\n",
    "      \"description\": {{ \"value\": \"...\", \"confidence\": 0.6 }},\n",
    "      \"duration_from\": {{ \"value\": \"2023-01\", \"confidence\": 0.9 }},\n",
    "      \"duration_to\": {{ \"value\": null, \"confidence\": 0.0 }}\n",
    "    }}\n",
    "  ],\n",
    "  \"section_confidence\": 0.9\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31849cf8",
   "metadata": {},
   "source": [
    "## Cell 8 â€” Section Parsing Functions (Pure & Stateless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7f7858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš¡ Parsing EXPERIENCE (1855 chars)...\n",
      "\n",
      "âœ… Test Parse Experience Result:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"companyName\": {\n",
      "        \"value\": \"Edubuk\",\n",
      "        \"confidence\": 0.95\n",
      "      },\n",
      "      \"jobRole\": {\n",
      "        \"value\": \"AI Engineer Intern\",\n",
      "        \"confidence\": 0.95\n",
      "      },\n",
      "      \"skills\": {\n",
      "        \"value\": \"Azure, OpenAI, Elasticsearch\",\n",
      "        \"confidence\": 0.8\n",
      "      },\n",
      "      \"description\": {\n",
      "        \"value\": \"- Built an AI-powered JD\\u2013CV matching system combining vector similarity for accuracy and LLM-based analysis for contextual understanding, enabling high-precision candidate\\u2013job matching at scale.\\n- Designed and deployed a serverless architecture on Azure using Azure Functions, including migration from an earlier AWS Lambda\\u2013based serverless setup, ensuring improved scalability and operational consistency.\\n- Implemented a Retrieval-Augmented pipeline using OpenAI text-embedding-3-large for semantic embeddings and GPT-4 for deep context evaluation between job descriptions and resumes.\\n- Migrated the search infrastructure from AWS OpenSearch to self-hosted Elasticsearch on Azure Virtual Machines, improving system control, flexibility, and long-term cost efficiency.\\n- Engineered backend services for resume ingestion, JD processing, and matching orchestration, delivering fast, reliable responses suitable for production-grade hiring workflows.\",\n",
      "        \"confidence\": 0.85\n",
      "      },\n",
      "      \"duration_from\": {\n",
      "        \"value\": \"2025-08\",\n",
      "        \"confidence\": 0.9\n",
      "      },\n",
      "      \"duration_to\": {\n",
      "        \"value\": \"Present\",\n",
      "        \"confidence\": 0.9\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"companyName\": {\n",
      "        \"value\": \"QTechSolutions\",\n",
      "        \"confidence\": 0.95\n",
      "      },\n",
      "      \"jobRole\": {\n",
      "        \"value\": \"AI/GenAi Intern\",\n",
      "        \"confidence\": 0.95\n",
      "      },\n",
      "      \"skills\": {\n",
      "        \"value\": \"NLP, machine learning\",\n",
      "        \"confidence\": 0.7\n",
      "      },\n",
      "      \"description\": {\n",
      "        \"value\": \"- Working on a real-world healthcare site to develop and integrate an AI-powered chatbot for doctor consultations, medicine delivery, and prescription recommendations.\\n- Implementing NLP and machine learning techniques to enhance chatbot accuracy, ensuring seamless and efficient user interactions in a scalable AI-driven system.\\n- Leveraging Retrieval-Augmented Generation (RAG) to provide dynamic, context-aware responses by combining LLM capabilities with real-time retrieval from healthcare databases.\",\n",
      "        \"confidence\": 0.85\n",
      "      },\n",
      "      \"duration_from\": {\n",
      "        \"value\": \"2023-03\",\n",
      "        \"confidence\": 0.9\n",
      "      },\n",
      "      \"duration_to\": {\n",
      "        \"value\": \"2025-08\",\n",
      "        \"confidence\": 0.9\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"companyName\": {\n",
      "        \"value\": \"Unified Mentor\",\n",
      "        \"confidence\": 0.95\n",
      "      },\n",
      "      \"jobRole\": {\n",
      "        \"value\": \"Data Science Intern\",\n",
      "        \"confidence\": 0.95\n",
      "      },\n",
      "      \"skills\": {\n",
      "        \"value\": \"NLP, sentiment analysis, topic modeling\",\n",
      "        \"confidence\": 0.7\n",
      "      },\n",
      "      \"description\": {\n",
      "        \"value\": \"- Implemented a text classification pipeline, improving document categorization accuracy by 20%\\n- Developed an NLP-based climate change analysis system using sentiment analysis and topic modeling to extract insights from global news articles.\",\n",
      "        \"confidence\": 0.85\n",
      "      },\n",
      "      \"duration_from\": {\n",
      "        \"value\": \"2024-11\",\n",
      "        \"confidence\": 0.9\n",
      "      },\n",
      "      \"duration_to\": {\n",
      "        \"value\": \"2025-01\",\n",
      "        \"confidence\": 0.9\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"section_confidence\": 0.9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ðŸŸ¦ CELL 8 â€” Section Parsing Functions (Pure & Stateless)\n",
    "\n",
    "def _fetch_parsed_data(section_name: str, text: str, model: Any, prompt: str) -> Any:\n",
    "    \"\"\"\n",
    "    Generic helper to call LLM with structured output.\n",
    "    Returns the Pydantic model instance (e.g., ParsedEducation).\n",
    "    \"\"\"\n",
    "    # 1. Fail fast if text is empty (save API cost)\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        print(f\"   â© Skipping {section_name}: Text empty or too short.\")\n",
    "        # Return an empty instance with 0 confidence\n",
    "        return model(section_confidence=0.0)\n",
    "\n",
    "    print(f\"   âš¡ Parsing {section_name.upper()} ({len(text)} chars)...\")\n",
    "    \n",
    "    # 2. Bind the specific Pydantic model to the LLM\n",
    "    structured_llm = llm.with_structured_output(model)\n",
    "    \n",
    "    try:\n",
    "        # 3. Invoke with the specific Prompt from Cell 7\n",
    "        response = structured_llm.invoke([\n",
    "            SystemMessage(content=prompt),\n",
    "            HumanMessage(content=f\"SECTION TEXT:\\n{text}\")\n",
    "        ])\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error parsing {section_name}: {e}\")\n",
    "        # Return safe fallback on crash\n",
    "        return model(section_confidence=0.0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Personal\n",
    "# ------------------------------------------------------------------\n",
    "def parse_personal(text: str) -> ParsedPersonal:\n",
    "    return _fetch_parsed_data(\"personal\", text, ParsedPersonal, PROMPT_PERSONAL)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Education\n",
    "# ------------------------------------------------------------------\n",
    "def parse_education(text: str) -> ParsedEducation:\n",
    "    return _fetch_parsed_data(\"education\", text, ParsedEducation, PROMPT_EDUCATION)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Experience\n",
    "# ------------------------------------------------------------------\n",
    "def parse_experience(text: str) -> ParsedExperience:\n",
    "    return _fetch_parsed_data(\"experience\", text, ParsedExperience, PROMPT_EXPERIENCE)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Skills\n",
    "# ------------------------------------------------------------------\n",
    "def parse_skills(text: str) -> ParsedSkills:\n",
    "    return _fetch_parsed_data(\"skills\", text, ParsedSkills, PROMPT_SKILLS)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Projects\n",
    "# ------------------------------------------------------------------\n",
    "def parse_projects(text: str) -> ParsedProjects:\n",
    "    return _fetch_parsed_data(\"projects\", text, ParsedProjects, PROMPT_PROJECTS)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Awards\n",
    "# ------------------------------------------------------------------\n",
    "def parse_awards(text: str) -> ParsedAwards:\n",
    "    return _fetch_parsed_data(\"awards\", text, ParsedAwards, PROMPT_AWARDS)\n",
    "\n",
    "# ---- QUICK TEST (Sanity Check) ----\n",
    "# We test with the 'experience' text we extracted in Cell 6\n",
    "if \"resume_sections\" in globals() and resume_sections[\"experience\"]:\n",
    "    test_exp = parse_experience(resume_sections[\"experience\"])\n",
    "    print(\"\\nâœ… Test Parse Experience Result:\")\n",
    "    print(json.dumps(test_exp.model_dump(), indent=2))\n",
    "else:\n",
    "    print(\"âš ï¸ No experience text found in global state to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec707b7d",
   "metadata": {},
   "source": [
    "## CELL 9 â€” LangGraph State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f3027a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¦ CELL 9 â€” LangGraph State Definition\n",
    "\n",
    "# This class defines the schema of the shared memory used by the graph.\n",
    "# Every node receives this state, modifies it, and passes it on.\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    # 1. RAW INPUTS\n",
    "    raw_text: str                   # The clean text from Cell 5\n",
    "    \n",
    "    # 2. INTERMEDIATE (The \"Router\" Output)\n",
    "    sections: Dict[str, str]        # The 6 semantic buckets from Cell 6\n",
    "    \n",
    "    # 3. AI PARSING RESULTS (The Pydantic Models from Cell 3)\n",
    "    # These start as None and get filled by the parallel workers\n",
    "    personal: Optional[ParsedPersonal]\n",
    "    education: Optional[ParsedEducation]\n",
    "    experience: Optional[ParsedExperience]\n",
    "    skills: Optional[ParsedSkills]\n",
    "    projects: Optional[ParsedProjects]\n",
    "    awards: Optional[ParsedAwards]\n",
    "    \n",
    "    # 4. ORCHESTRATION & CONTROL\n",
    "    # Tracks how confident we are about each section\n",
    "    confidence_map: Dict[str, float] \n",
    "    \n",
    "    # Tracks how many times we have retried a specific section\n",
    "    retry_counts: Dict[str, int]\n",
    "    \n",
    "    # 5. FINAL OUTPUT\n",
    "    # The \"Draft JSON\" that matches newCv.model.ts (Cell 2)\n",
    "    trucv_draft: Optional[Dict[str, Any]] \n",
    "    \n",
    "    # 6. LOGS\n",
    "    errors: List[str]               # Accumulates error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9432c82",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 10 â€” Confidence Scoring Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e31c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¦ CELL 10 â€” Confidence Scoring Engine (Business Logic Layer)\n",
    "\n",
    "def calculate_personal_score(personal: ParsedPersonal) -> (float, List[str]):\n",
    "    \"\"\"Calculates weighted confidence for Personal section.\"\"\"\n",
    "    score = 1.0\n",
    "    warnings = []\n",
    "    \n",
    "    # CRITICAL FIELDS (Heavy Penalty)\n",
    "    if not personal.fullName.value:\n",
    "        score -= 0.4\n",
    "        warnings.append(\"CRITICAL: Name is missing.\")\n",
    "    if not personal.email.value:\n",
    "        score -= 0.3\n",
    "        warnings.append(\"CRITICAL: Email is missing.\")\n",
    "    if not personal.phone.value:\n",
    "        score -= 0.1\n",
    "        warnings.append(\"WARNING: Phone number is missing.\")\n",
    "        \n",
    "    # ENRICHMENT FIELDS (Light Penalty)\n",
    "    if not personal.linkedin.value:\n",
    "        score -= 0.05\n",
    "        warnings.append(\"INFO: LinkedIn profile not found.\")\n",
    "    if not personal.city.value:\n",
    "        score -= 0.05\n",
    "    \n",
    "    # Average in the AI's own confidence for the found fields\n",
    "    ai_conf = (personal.fullName.confidence + personal.email.confidence) / 2\n",
    "    final_score = (score * 0.7) + (ai_conf * 0.3)\n",
    "    \n",
    "    return max(0.0, final_score), warnings\n",
    "\n",
    "def calculate_education_score(education: ParsedEducation) -> (float, List[str]):\n",
    "    \"\"\"Checks for missing dates, institution names, and degrees.\"\"\"\n",
    "    if not education.items:\n",
    "        return 0.0, [\"WARNING: No education entries found.\"]\n",
    "    \n",
    "    total_score = 0.0\n",
    "    warnings = []\n",
    "    \n",
    "    for i, item in enumerate(education.items):\n",
    "        item_score = 1.0\n",
    "        \n",
    "        # Check Criticals\n",
    "        if not item.institutionName.value:\n",
    "            item_score -= 0.3\n",
    "            warnings.append(f\"Edu #{i+1}: Institution name missing.\")\n",
    "        if not item.boardNameOrDegree.value:\n",
    "            item_score -= 0.2\n",
    "            warnings.append(f\"Edu #{i+1}: Degree/Board missing.\")\n",
    "            \n",
    "        # Check Dates (Critical for Verification)\n",
    "        if not item.duration_from.value:\n",
    "            item_score -= 0.15\n",
    "            warnings.append(f\"Edu #{i+1}: Start date missing.\")\n",
    "            \n",
    "        # Check GPA (Useful but not critical)\n",
    "        if not item.gpa.value:\n",
    "            warnings.append(f\"Edu #{i+1}: GPA/Percentage missing.\")\n",
    "            \n",
    "        total_score += item_score\n",
    "\n",
    "    avg_score = total_score / len(education.items)\n",
    "    return max(0.0, avg_score), warnings\n",
    "\n",
    "def calculate_experience_score(experience: ParsedExperience) -> (float, List[str]):\n",
    "    if not experience.items:\n",
    "        # It's okay for freshers to have no experience, but we flag it\n",
    "        return 1.0, [\"INFO: No experience detected (Fresher?).\"]\n",
    "    \n",
    "    total_score = 0.0\n",
    "    warnings = []\n",
    "    \n",
    "    for i, item in enumerate(experience.items):\n",
    "        item_score = 1.0\n",
    "        \n",
    "        if not item.companyName.value:\n",
    "            item_score -= 0.3\n",
    "            warnings.append(f\"Exp #{i+1}: Company name missing.\")\n",
    "        if not item.jobRole.value:\n",
    "            item_score -= 0.2\n",
    "            warnings.append(f\"Exp #{i+1}: Job role missing.\")\n",
    "            \n",
    "        # Description check\n",
    "        desc = item.description.value or \"\"\n",
    "        if len(desc) < 20:\n",
    "            item_score -= 0.1\n",
    "            warnings.append(f\"Exp #{i+1}: Description is too short or empty.\")\n",
    "            \n",
    "        total_score += item_score\n",
    "\n",
    "    avg_score = total_score / len(experience.items)\n",
    "    return max(0.0, avg_score), warnings\n",
    "\n",
    "# ---- MAIN NODE FUNCTION ----\n",
    "\n",
    "def node_confidence_scoring(state: GraphState):\n",
    "    print(\"ðŸ“Š [Scoring] Analyzing extracted data quality...\")\n",
    "    \n",
    "    confidence_map = {}\n",
    "    all_warnings = []\n",
    "    \n",
    "    # 1. Personal\n",
    "    if state[\"personal\"]:\n",
    "        p_score, p_warn = calculate_personal_score(state[\"personal\"])\n",
    "        confidence_map[\"personal\"] = round(p_score, 2)\n",
    "        all_warnings.extend(p_warn)\n",
    "    else:\n",
    "        confidence_map[\"personal\"] = 0.0\n",
    "        all_warnings.append(\"CRITICAL: Personal section failed to parse.\")\n",
    "\n",
    "    # 2. Education\n",
    "    if state[\"education\"]:\n",
    "        e_score, e_warn = calculate_education_score(state[\"education\"])\n",
    "        confidence_map[\"education\"] = round(e_score, 2)\n",
    "        all_warnings.extend(e_warn)\n",
    "        \n",
    "    # 3. Experience\n",
    "    if state[\"experience\"]:\n",
    "        exp_score, exp_warn = calculate_experience_score(state[\"experience\"])\n",
    "        confidence_map[\"experience\"] = round(exp_score, 2)\n",
    "        all_warnings.extend(exp_warn)\n",
    "        \n",
    "    # (We can add Skills/Projects scoring similarly, but these 3 are core)\n",
    "    \n",
    "    # Update state\n",
    "    # We append new warnings to any existing errors\n",
    "    current_errors = state.get(\"errors\", [])\n",
    "    current_errors.extend(all_warnings)\n",
    "    \n",
    "    print(f\"   Scores: {confidence_map}\")\n",
    "    if all_warnings:\n",
    "        print(f\"   Warnings: {len(all_warnings)} detected.\")\n",
    "\n",
    "    return {\n",
    "        \"confidence_map\": confidence_map,\n",
    "        \"errors\": current_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848eed6",
   "metadata": {},
   "source": [
    "## CELL 11 â€” Retry Low-Confidence Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6142a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¦ CELL 11 â€” Retry Logic (Self-Correction Layer)\n",
    "\n",
    "def node_retry_logic(state: GraphState):\n",
    "    \"\"\"\n",
    "    Checks confidence scores. If any section is below threshold,\n",
    "    it triggers an immediate re-parse with a stricter prompt.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ [Retry] Evaluating need for self-correction...\")\n",
    "    \n",
    "    # Get current state data\n",
    "    conf_map = state.get(\"confidence_map\", {})\n",
    "    sections = state[\"sections\"]\n",
    "    \n",
    "    # We will build a dictionary of updates\n",
    "    updates = {}\n",
    "    new_logs = []\n",
    "    \n",
    "    # List of sections to check\n",
    "    # (Matches keys in state and confidence_map)\n",
    "    check_list = [\"personal\", \"education\", \"experience\", \"skills\", \"projects\", \"awards\"]\n",
    "    \n",
    "    for section in check_list:\n",
    "        score = conf_map.get(section, 0.0)\n",
    "        \n",
    "        # 1. Check if score is unacceptable\n",
    "        if score < CONFIDENCE_THRESHOLD and score > 0.0:\n",
    "            print(f\"   âš ï¸ Low confidence in '{section}' ({score}). Attempting Retry...\")\n",
    "            \n",
    "            # 2. Get the specific text again\n",
    "            text = sections.get(section, \"\")\n",
    "            \n",
    "            # 3. Construct a \"Repair Prompt\"\n",
    "            # We add a specific instruction to be more careful\n",
    "            repair_instruction = f\"\"\"\n",
    "            PREVIOUS ATTEMPT FAILED. \n",
    "            The confidence score was low ({score}).\n",
    "            \n",
    "            CRITICAL INSTRUCTION:\n",
    "            - Look closer for missing dates or names.\n",
    "            - If the text is truly empty/missing, return null values with 0 confidence.\n",
    "            - Do not guess.\n",
    "            \"\"\"\n",
    "            \n",
    "            # 4. Select the correct parsing function & model dynamically\n",
    "            if section == \"personal\":\n",
    "                # We append the repair instruction to the original prompt\n",
    "                new_result = _fetch_parsed_data(\"personal\", text, ParsedPersonal, PROMPT_PERSONAL + repair_instruction)\n",
    "                updates[\"personal\"] = new_result\n",
    "                \n",
    "            elif section == \"education\":\n",
    "                new_result = _fetch_parsed_data(\"education\", text, ParsedEducation, PROMPT_EDUCATION + repair_instruction)\n",
    "                updates[\"education\"] = new_result\n",
    "\n",
    "            elif section == \"experience\":\n",
    "                new_result = _fetch_parsed_data(\"experience\", text, ParsedExperience, PROMPT_EXPERIENCE + repair_instruction)\n",
    "                updates[\"experience\"] = new_result\n",
    "                \n",
    "            elif section == \"skills\":\n",
    "                new_result = _fetch_parsed_data(\"skills\", text, ParsedSkills, PROMPT_SKILLS + repair_instruction)\n",
    "                updates[\"skills\"] = new_result\n",
    "                \n",
    "            elif section == \"projects\":\n",
    "                new_result = _fetch_parsed_data(\"projects\", text, ParsedProjects, PROMPT_PROJECTS + repair_instruction)\n",
    "                updates[\"projects\"] = new_result\n",
    "            \n",
    "            elif section == \"awards\":\n",
    "                new_result = _fetch_parsed_data(\"awards\", text, ParsedAwards, PROMPT_AWARDS + repair_instruction)\n",
    "                updates[\"awards\"] = new_result\n",
    "            \n",
    "            new_logs.append(f\"RETRY: Re-parsed {section} (Prev Score: {score})\")\n",
    "            \n",
    "        else:\n",
    "            # Score is good, or section is empty (0.0). No action.\n",
    "            pass\n",
    "\n",
    "    if not updates:\n",
    "        print(\"   âœ… No retries needed. Quality is sufficient.\")\n",
    "    else:\n",
    "        print(f\"   âœ… Retried {len(updates)} sections.\")\n",
    "        # We append the log of what we did\n",
    "        current_errors = state.get(\"errors\", [])\n",
    "        current_errors.extend(new_logs)\n",
    "        updates[\"errors\"] = current_errors\n",
    "\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857749a",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 12 â€” Assemble TruCV Draft (Deterministic Mapping Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¦ CELL 12 â€” Assemble TruCV Draft (Deterministic Mapping Layer)\n",
    "\n",
    "def safe_val(field: Optional[ParsedField]) -> str:\n",
    "    \"\"\"Helper: Extracts string value from ParsedField, defaulting to empty string.\"\"\"\n",
    "    if field and field.value:\n",
    "        return field.value.strip()\n",
    "    return \"\"\n",
    "\n",
    "def create_duration(start: Optional[ParsedField], end: Optional[ParsedField]) -> Duration:\n",
    "    \"\"\"Helper: Creates a TruCV Duration object.\"\"\"\n",
    "    return Duration(\n",
    "        from_=safe_val(start),\n",
    "        to=safe_val(end)\n",
    "    )\n",
    "\n",
    "def node_assemble_draft(state: GraphState):\n",
    "    print(\"ðŸ—ï¸ [Assembly] Mapping AI data to TruCV Schema...\")\n",
    "    \n",
    "    # 1. SETUP DEFAULTS\n",
    "    # In a real app, userId comes from the request context\n",
    "    user_id_mock = \"user_12345_mock_id\" \n",
    "    cv_title = \"Uploaded Resume Draft\"\n",
    "\n",
    "    # 2. MAP PERSONAL SECTION\n",
    "    p_data = state.get(\"personal\")\n",
    "    personal_obj = PersonalInfo(\n",
    "        fullName=safe_val(p_data.fullName) if p_data else \"\",\n",
    "        email=safe_val(p_data.email) if p_data else \"\",\n",
    "        phone=safe_val(p_data.phone) if p_data else \"\",\n",
    "        city=safe_val(p_data.city) if p_data else \"\",\n",
    "        linkedin=safe_val(p_data.linkedin) if p_data else \"\",\n",
    "        github=safe_val(p_data.github) if p_data else \"\",\n",
    "        summary=safe_val(p_data.summary) if p_data else \"\",\n",
    "        imgUrl=\"\" # AI doesn't extract images yet\n",
    "    )\n",
    "\n",
    "    # 3. MAP EDUCATION\n",
    "    educations_list = []\n",
    "    if state.get(\"education\") and state[\"education\"].items:\n",
    "        for item in state[\"education\"].items:\n",
    "            educations_list.append(Education(\n",
    "                id=str(uuid.uuid4()), # Generate distinct ID for frontend keys\n",
    "                eduDocId=str(uuid.uuid4()), # Placeholder for DB requirement\n",
    "                level=safe_val(item.level),\n",
    "                boardNameOrDegree=safe_val(item.boardNameOrDegree),\n",
    "                institutionName=safe_val(item.institutionName),\n",
    "                gpa=safe_val(item.gpa),\n",
    "                duration=create_duration(item.duration_from, item.duration_to),\n",
    "                # Defaults enforced by Schema in Cell 2\n",
    "                selfAttested=True,\n",
    "                verified=False,\n",
    "                status=\"pending\"\n",
    "            ))\n",
    "\n",
    "    # 4. MAP EXPERIENCE\n",
    "    experiences_list = []\n",
    "    if state.get(\"experience\") and state[\"experience\"].items:\n",
    "        for item in state[\"experience\"].items:\n",
    "            experiences_list.append(Experience(\n",
    "                id=str(uuid.uuid4()),\n",
    "                companyName=safe_val(item.companyName),\n",
    "                jobRole=safe_val(item.jobRole),\n",
    "                description=safe_val(item.description),\n",
    "                skills=safe_val(item.skills), # AI returns comma-separated string, matching DB\n",
    "                duration=create_duration(item.duration_from, item.duration_to),\n",
    "                selfAttested=True,\n",
    "                verified=False,\n",
    "                status=\"pending\"\n",
    "            ))\n",
    "\n",
    "    # 5. MAP SKILLS\n",
    "    skills_list = []\n",
    "    if state.get(\"skills\") and state[\"skills\"].items:\n",
    "        for item in state[\"skills\"].items:\n",
    "            skills_list.append(Skill(\n",
    "                id=str(uuid.uuid4()),\n",
    "                skillName=safe_val(item.skillName),\n",
    "                level=safe_val(item.level) or \"Intermediate\",\n",
    "                selfAttested=True\n",
    "            ))\n",
    "\n",
    "    # 6. MAP PROJECTS\n",
    "    projects_list = []\n",
    "    if state.get(\"projects\") and state[\"projects\"].items:\n",
    "        for item in state[\"projects\"].items:\n",
    "            projects_list.append(Project(\n",
    "                id=str(uuid.uuid4()),\n",
    "                projectName=safe_val(item.projectName),\n",
    "                projectUrl=safe_val(item.projectUrl),\n",
    "                description=safe_val(item.description),\n",
    "                skills=safe_val(item.skills),\n",
    "                duration=create_duration(item.duration_from, item.duration_to),\n",
    "                selfAttested=True\n",
    "            ))\n",
    "\n",
    "    # 7. MAP AWARDS\n",
    "    awards_list = []\n",
    "    if state.get(\"awards\") and state[\"awards\"].items:\n",
    "        for item in state[\"awards\"].items:\n",
    "            awards_list.append(Award(\n",
    "                id=str(uuid.uuid4()),\n",
    "                name=safe_val(item.name),\n",
    "                organisation=safe_val(item.organisation),\n",
    "                level=safe_val(item.level),\n",
    "                description=safe_val(item.description),\n",
    "                duration=create_duration(item.duration_from, item.duration_to),\n",
    "                selfAttested=True,\n",
    "                verified=False,\n",
    "                status=\"pending\"\n",
    "            ))\n",
    "\n",
    "    # 8. CONSTRUCT ROOT OBJECT\n",
    "    final_draft = TruCVDraft(\n",
    "        userId=user_id_mock,\n",
    "        title=cv_title,\n",
    "        personal=personal_obj,\n",
    "        educations=educations_list,\n",
    "        experiences=experiences_list,\n",
    "        skills=skills_list,\n",
    "        projects=projects_list,\n",
    "        awards=awards_list\n",
    "    )\n",
    "\n",
    "    # 9. VALIDATION & RETURN\n",
    "    # This automatically validates against Pydantic rules from Cell 2\n",
    "    try:\n",
    "        draft_dict = final_draft.model_dump(by_alias=True)\n",
    "        print(f\"âœ… Assembly Successful! Generated {len(educations_list)} Edu, {len(experiences_list)} Exp entries.\")\n",
    "        return {\"trucv_draft\": draft_dict}\n",
    "    except ValidationError as e:\n",
    "        print(f\"âŒ Validation Error during assembly: {e}\")\n",
    "        return {\"errors\": [str(e)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b823508",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ CELL 13 â€” Final Graph Compilation & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ba2d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ STARTING FINAL TRUCV PIPELINE...\n",
      "ðŸš¦ [Router] Splitting resume into sections...\n",
      "ðŸ§  Detecting sections semantically (No Regex)...\n",
      "   ðŸ† [Awards] Parsing...\n",
      "   âš¡ Parsing AWARDS (265 chars)...\n",
      "   ðŸŽ“ [Education] Parsing...\n",
      "   âš¡ Parsing EDUCATION (219 chars)...\n",
      "   ðŸ’¼ [Experience] Parsing...\n",
      "   âš¡ Parsing EXPERIENCE (1855 chars)...\n",
      "   ðŸ‘¤ [Personal] Parsing...\n",
      "   âš¡ Parsing PERSONAL (102 chars)...\n",
      "   ðŸš€ [Projects] Parsing...\n",
      "   âš¡ Parsing PROJECTS (1427 chars)...\n",
      "   ðŸ› ï¸ [Skills] Parsing...\n",
      "   âš¡ Parsing SKILLS (980 chars)...\n",
      "ðŸ“Š [Scoring] Analyzing extracted data quality...\n",
      "   Scores: {'personal': 0.96, 'education': 1.0, 'experience': 1.0}\n",
      "   Warnings: 1 detected.\n",
      "ðŸ”„ [Retry] Evaluating need for self-correction...\n",
      "   âœ… No retries needed. Quality is sufficient.\n",
      "ðŸ—ï¸ [Assembly] Mapping AI data to TruCV Schema...\n",
      "âœ… Assembly Successful! Generated 2 Edu, 3 Exp entries.\n",
      "\n",
      "ðŸ PIPELINE FINISHED.\n",
      "\n",
      "âœ… FINAL JSON OUTPUT (Ready for MongoDB):\n",
      "{\n",
      "  \"userId\": \"user_12345_mock_id\",\n",
      "  \"title\": \"Uploaded Resume Draft\",\n",
      "  \"personal\": {\n",
      "    \"fullName\": \"Ganesh Agrahari\",\n",
      "    \"email\": \"ganeshagrahari08@gmail.com\",\n",
      "    \"phone\": \"+91 9044232872\",\n",
      "    \"city\": \"Lucknow, India\",\n",
      "    \"linkedin\": \"\",\n",
      "    \"github\": \"\",\n",
      "    \"summary\": \"\",\n",
      "    \"imgUrl\": \"\"\n",
      "  },\n",
      "  \"educations\": [\n",
      "    {\n",
      "      \"id\": \"e6f86a6d-8b55-4a2f-ad5a-43e0be5e0e29\",\n",
      "      \"eduDocId\": \"f9af1c3c-dced-4d26-ac3f-5cf73b32a945\",\n",
      "      \"level\": \"Undergraduate\",\n",
      "      \"boardNameOrDegree\": \"BCA Data Science & Artificial Intelligence\",\n",
      "      \"institutionName\": \"BBD University\",\n",
      "      \"gpa\": \"8\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"2023-08\",\n",
      "        \"to\": \"2026-09\"\n",
      "      },\n",
      "      \"selfAttested\": true,\n",
      "      \"docUri\": null,\n",
      "      \"issuerEmailId\": \"\",\n",
      "      \"isEmailSend\": false,\n",
      "      \"verified\": false,\n",
      "      \"status\": \"pending\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"3a4efcf9-201d-48da-ba11-dbea5fd04c3d\",\n",
      "      \"eduDocId\": \"aa321d64-cfa0-4f6b-ae50-23fd3a0c7a94\",\n",
      "      \"level\": \"Grade 12\",\n",
      "      \"boardNameOrDegree\": \"Intermediate(PCM)\",\n",
      "      \"institutionName\": \"SVM Inter College Ntpc\",\n",
      "      \"gpa\": \"82%\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"2022\",\n",
      "        \"to\": \"2022\"\n",
      "      },\n",
      "      \"selfAttested\": true,\n",
      "      \"docUri\": null,\n",
      "      \"issuerEmailId\": \"\",\n",
      "      \"isEmailSend\": false,\n",
      "      \"verified\": false,\n",
      "      \"status\": \"pending\"\n",
      "    }\n",
      "  ],\n",
      "  \"experiences\": [\n",
      "    {\n",
      "      \"id\": \"c2f2b750-0c94-472f-ae0d-ebf9aabc5adc\",\n",
      "      \"companyName\": \"Edubuk\",\n",
      "      \"jobRole\": \"AI Engineer Intern\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"2025-08\",\n",
      "        \"to\": \"Present\"\n",
      "      },\n",
      "      \"skills\": \"Azure, OpenAI, Elasticsearch\",\n",
      "      \"description\": \"- Built an AI-powered JD\\u2013CV matching system combining vector similarity for accuracy and LLM-based analysis for contextual understanding, enabling high-precision candidate\\u2013job matching at scale.\\n- Designed and deployed a serverless architecture on Azure using Azure Functions, including migration from an earlier AWS Lambda\\u2013based serverless setup, ensuring improved scalability and operational consistency.\\n- Implemented a Retrieval-Augmented pipeline using OpenAI text-embedding-3-large for semantic embeddings and GPT-4 for deep context evaluation between job descriptions and resumes.\\n- Migrated the search infrastructure from AWS OpenSearch to self-hosted Elasticsearch on Azure Virtual Machines, improving system control, flexibility, and long-term cost efficiency.\\n- Engineered backend services for resume ingestion, JD processing, and matching orchestration, delivering fast, reliable responses suitable for production-grade hiring workflows.\",\n",
      "      \"selfAttested\": true,\n",
      "      \"isEmailSend\": false,\n",
      "      \"docUri\": null,\n",
      "      \"issuerEmailId\": \"\",\n",
      "      \"verified\": false,\n",
      "      \"status\": \"pending\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"e127041c-6dfd-4622-91e5-c2424ff6f847\",\n",
      "      \"companyName\": \"QTechSolutions\",\n",
      "      \"jobRole\": \"AI/GenAi Intern\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"2023-03\",\n",
      "        \"to\": \"2025-08\"\n",
      "      },\n",
      "      \"skills\": \"NLP, machine learning\",\n",
      "      \"description\": \"- Working on a real-world healthcare site to develop and integrate an AI-powered chatbot for doctor consultations, medicine delivery, and prescription recommendations.\\n- Implementing NLP and machine learning techniques to enhance chatbot accuracy, ensuring seamless and efficient user interactions in a scalable AI-driven system.\\n- Leveraging Retrieval-Augmented Generation (RAG) to provide dynamic, context-aware responses by combining LLM capabilities with real-time retrieval from healthcare databases.\",\n",
      "      \"selfAttested\": true,\n",
      "      \"isEmailSend\": false,\n",
      "      \"docUri\": null,\n",
      "      \"issuerEmailId\": \"\",\n",
      "      \"verified\": false,\n",
      "      \"status\": \"pending\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"49e58069-63ec-48a5-b042-2fc1f86637dd\",\n",
      "      \"companyName\": \"Unified Mentor\",\n",
      "      \"jobRole\": \"Data Science Intern\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"2024-11\",\n",
      "        \"to\": \"2025-01\"\n",
      "      },\n",
      "      \"skills\": \"NLP, sentiment analysis, topic modeling\",\n",
      "      \"description\": \"- Implemented a text classification pipeline, improving document categorization accuracy by 20%\\n- Developed an NLP-based climate change analysis system using sentiment analysis and topic modeling to extract insights from global news articles.\",\n",
      "      \"selfAttested\": true,\n",
      "      \"isEmailSend\": false,\n",
      "      \"docUri\": null,\n",
      "      \"issuerEmailId\": \"\",\n",
      "      \"verified\": false,\n",
      "      \"status\": \"pending\"\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"id\": \"77d1fb8a-4c24-4826-b364-453ce1e436eb\",\n",
      "      \"skillName\": \"Python\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cb580814-b859-48cb-a465-1a3f0282f15a\",\n",
      "      \"skillName\": \"JavaScript\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"c8275362-8925-4452-975c-45096d7913c3\",\n",
      "      \"skillName\": \"Async Programming\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"5d6f3946-5782-4648-8c42-6140033938bb\",\n",
      "      \"skillName\": \"REST API Development\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"74a66a7f-0870-4555-b40b-be137a2d6c58\",\n",
      "      \"skillName\": \"Machine Learning\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"bf8a486a-af2b-4b98-a4e6-eff40fb766eb\",\n",
      "      \"skillName\": \"Deep Learning\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"d2b25d3d-c3fa-43b2-a87e-bb1bb2f80a5a\",\n",
      "      \"skillName\": \"NLP\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"f0e1f248-8251-447e-aef0-127effd6dde8\",\n",
      "      \"skillName\": \"LLMs\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ee2c1e57-8cdd-4bf1-9ca3-ea1d444bba31\",\n",
      "      \"skillName\": \"RAG\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"e1f14ee2-1ffb-4f6d-917c-04027cd7a33f\",\n",
      "      \"skillName\": \"Vector Search (Elasticsearch)\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"c763ef73-b7a7-43cc-a1cd-39eb4b6e3b80\",\n",
      "      \"skillName\": \"Multi-Vector Embeddings (3072-dim)\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"88a5efad-962b-4809-8e42-2d72a7a8620c\",\n",
      "      \"skillName\": \"Hybrid Scoring (GPT-4 + vector similarity)\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"92cfebf7-563d-401c-8271-4516f6309a6d\",\n",
      "      \"skillName\": \"Prompt Engineering\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"4727aa21-ba26-4e33-8fac-dcd0441ddfcb\",\n",
      "      \"skillName\": \"PyTorch\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ea4218f7-7fa0-4280-80a3-7b667e7eed7a\",\n",
      "      \"skillName\": \"LangChain\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"3e716bc8-ce04-47ac-9853-908e8eb0d78c\",\n",
      "      \"skillName\": \"LangGraph\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"9e2f6522-3b6f-4b0a-be0e-1b158825dc68\",\n",
      "      \"skillName\": \"Scikit-learn\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cf2d0491-36f7-43fa-acd6-e874a064ce6c\",\n",
      "      \"skillName\": \"OpenCV\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"fcbdf772-fe55-4913-a7b4-8f2bd9112419\",\n",
      "      \"skillName\": \"N8N\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"993d8670-6cf9-4d3d-a118-55467fa1cb46\",\n",
      "      \"skillName\": \"Azure\",\n",
      "      \"level\": \"Expert\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"37fdec19-043d-47aa-afa5-e6aee2f37f84\",\n",
      "      \"skillName\": \"AWS\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"f69dcb42-dc61-4e03-82a0-d8844476503e\",\n",
      "      \"skillName\": \"Microservices\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"5cf7cb8d-1d84-4b77-8f5e-aa5bb23b2f0d\",\n",
      "      \"skillName\": \"Serverless Architecture\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"b13cc52f-6ba8-456f-aafe-80079a14fc8c\",\n",
      "      \"skillName\": \"Async Pipelines\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"0ec65673-40d6-4ea2-a2ef-1047a80d8249\",\n",
      "      \"skillName\": \"CI/CD\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"88c1c844-6f0f-4f88-9500-4a493bf86101\",\n",
      "      \"skillName\": \"Git\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"3742d635-4fdc-4d47-a5d1-a85a9b3bcabe\",\n",
      "      \"skillName\": \"GitHub\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"fa9c7b93-48af-49b2-834a-a3df97d93101\",\n",
      "      \"skillName\": \"Data Processing\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cc4cc245-fe2f-4639-bfed-1493eeb52766\",\n",
      "      \"skillName\": \"ETL\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"6ea574c6-5a36-4975-84df-4734105e696d\",\n",
      "      \"skillName\": \"Feature Engineering\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"c78bd461-a7e5-45b3-99a2-42f9100f428b\",\n",
      "      \"skillName\": \"Power BI\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"4168805c-a2d6-4c6b-b3ad-657fdd7703b7\",\n",
      "      \"skillName\": \"Jupyter Notebook\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"6bc30908-eed2-48b6-9d93-d9d807ee628c\",\n",
      "      \"skillName\": \"Matplotlib\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"b78f9a2d-9c8e-4094-8455-c453e3f45180\",\n",
      "      \"skillName\": \"Seaborn\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"01fbb22e-28ad-44d6-afaa-c9faedcfe964\",\n",
      "      \"skillName\": \"SQL\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"12aa54f1-9438-4a2e-9466-835e24c0cf24\",\n",
      "      \"skillName\": \"Elasticsearch (vector + keyword fields)\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"5608f924-3b96-4523-9a5b-67f2140847af\",\n",
      "      \"skillName\": \"AWS S3\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"22e46920-f8e8-4b30-8f1d-c27d51a8ab37\",\n",
      "      \"skillName\": \"Azure Blob Storage\",\n",
      "      \"level\": \"Intermediate\",\n",
      "      \"selfAttested\": true,\n",
      "      \"endoresBy\": \"\",\n",
      "      \"endoresThrough\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"id\": \"a2bb749b-bdd1-450d-9231-cd23a0e10bc4\",\n",
      "      \"projectName\": \"AI Recruitment System\",\n",
      "      \"projectUrl\": \"https://edubuk.com/trujobs\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"\",\n",
      "        \"to\": \"\"\n",
      "      },\n",
      "      \"skills\": \"AI, OpenAI, Elasticsearch\",\n",
      "      \"description\": \"Designed an AI-driven JD\\u2013CV matching workflow that evaluates both semantic similarity and contextual relevance, improving hiring signal quality beyond keyword-based matching.\",\n",
      "      \"selfAttested\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"1942a296-6c6d-48a0-8de2-1c494670a05b\",\n",
      "      \"projectName\": \"Face Recognition Attendance System\",\n",
      "      \"projectUrl\": \"\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"\",\n",
      "        \"to\": \"\"\n",
      "      },\n",
      "      \"skills\": \"Python, OpenCV, CNN\",\n",
      "      \"description\": \"Designed and deployed a real-time face recognition attendance system using Python, OpenCV, and CNN, achieving 95%+ accuracy and reducing manual attendance tracking efforts by over 40%.\",\n",
      "      \"selfAttested\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"71aaec6f-7bb2-4d4d-b97e-1e8f2d513cba\",\n",
      "      \"projectName\": \"Next.js Portfolio site\",\n",
      "      \"projectUrl\": \"https://vercel.com/nextjs-portfolio\",\n",
      "      \"duration\": {\n",
      "        \"from\": \"\",\n",
      "        \"to\": \"\"\n",
      "      },\n",
      "      \"skills\": \"Next.js, Vercel\",\n",
      "      \"description\": \"Interactive Portfolio Website - Built using Next.js and deployed on Vercel, featuring a responsive design and an AI-powered chatbot for seamless user interaction.\",\n",
      "      \"selfAttested\": true\n",
      "    }\n",
      "  ],\n",
      "  \"awards\": []\n",
      "}\n",
      "\n",
      "ðŸ’¾ Saved to 'trucv_draft.json'\n"
     ]
    }
   ],
   "source": [
    "# ðŸŸ¦ CELL 13 â€” Final Graph Compilation & Execution\n",
    "# This runs AFTER all logic cells (8, 11, 12, 13) are defined.\n",
    "\n",
    "# ==========================================\n",
    "# 1. DEFINE PARSER NODES (Wrappers)\n",
    "# ==========================================\n",
    "# These wrapper functions bridge the gap between \"GraphState\" and your \"Pure Functions\"\n",
    "\n",
    "def node_detect_sections(state: GraphState):\n",
    "    print(\"ðŸš¦ [Router] Splitting resume into sections...\")\n",
    "    return {\"sections\": detect_sections_semantically(state[\"raw_text\"])}\n",
    "\n",
    "def node_parse_personal(state: GraphState):\n",
    "    print(\"   ðŸ‘¤ [Personal] Parsing...\")\n",
    "    return {\"personal\": parse_personal(state[\"sections\"].get(\"personal\", \"\"))}\n",
    "\n",
    "def node_parse_education(state: GraphState):\n",
    "    print(\"   ðŸŽ“ [Education] Parsing...\")\n",
    "    return {\"education\": parse_education(state[\"sections\"].get(\"education\", \"\"))}\n",
    "\n",
    "def node_parse_experience(state: GraphState):\n",
    "    print(\"   ðŸ’¼ [Experience] Parsing...\")\n",
    "    return {\"experience\": parse_experience(state[\"sections\"].get(\"experience\", \"\"))}\n",
    "\n",
    "def node_parse_skills(state: GraphState):\n",
    "    print(\"   ðŸ› ï¸ [Skills] Parsing...\")\n",
    "    return {\"skills\": parse_skills(state[\"sections\"].get(\"skills\", \"\"))}\n",
    "\n",
    "def node_parse_projects(state: GraphState):\n",
    "    print(\"   ðŸš€ [Projects] Parsing...\")\n",
    "    return {\"projects\": parse_projects(state[\"sections\"].get(\"projects\", \"\"))}\n",
    "\n",
    "def node_parse_awards(state: GraphState):\n",
    "    print(\"   ðŸ† [Awards] Parsing...\")\n",
    "    return {\"awards\": parse_awards(state[\"sections\"].get(\"awards\", \"\"))}\n",
    "\n",
    "# NOTE: node_confidence_scoring, node_retry_logic, and node_assemble_draft \n",
    "# are ALREADY defined in Cells 11, 12, and 13. We use them directly below.\n",
    "\n",
    "# ==========================================\n",
    "# 2. BUILD THE GRAPH\n",
    "# ==========================================\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# A. Add All Nodes\n",
    "workflow.add_node(\"detect_sections\", node_detect_sections)\n",
    "workflow.add_node(\"parse_personal\", node_parse_personal)\n",
    "workflow.add_node(\"parse_education\", node_parse_education)\n",
    "workflow.add_node(\"parse_experience\", node_parse_experience)\n",
    "workflow.add_node(\"parse_skills\", node_parse_skills)\n",
    "workflow.add_node(\"parse_projects\", node_parse_projects)\n",
    "workflow.add_node(\"parse_awards\", node_parse_awards)\n",
    "\n",
    "# The Logic Nodes from Cells 11, 12, 13\n",
    "workflow.add_node(\"confidence_scoring\", node_confidence_scoring)\n",
    "workflow.add_node(\"retry_logic\", node_retry_logic)\n",
    "workflow.add_node(\"assemble_draft\", node_assemble_draft)\n",
    "\n",
    "# B. Wire the Edges\n",
    "workflow.add_edge(START, \"detect_sections\")\n",
    "\n",
    "# Fan-Out\n",
    "workflow.add_edge(\"detect_sections\", \"parse_personal\")\n",
    "workflow.add_edge(\"detect_sections\", \"parse_education\")\n",
    "workflow.add_edge(\"detect_sections\", \"parse_experience\")\n",
    "workflow.add_edge(\"detect_sections\", \"parse_skills\")\n",
    "workflow.add_edge(\"detect_sections\", \"parse_projects\")\n",
    "workflow.add_edge(\"detect_sections\", \"parse_awards\")\n",
    "\n",
    "# Fan-In\n",
    "workflow.add_edge(\"parse_personal\", \"confidence_scoring\")\n",
    "workflow.add_edge(\"parse_education\", \"confidence_scoring\")\n",
    "workflow.add_edge(\"parse_experience\", \"confidence_scoring\")\n",
    "workflow.add_edge(\"parse_skills\", \"confidence_scoring\")\n",
    "workflow.add_edge(\"parse_projects\", \"confidence_scoring\")\n",
    "workflow.add_edge(\"parse_awards\", \"confidence_scoring\")\n",
    "\n",
    "# Linear Finish\n",
    "workflow.add_edge(\"confidence_scoring\", \"retry_logic\")\n",
    "workflow.add_edge(\"retry_logic\", \"assemble_draft\")\n",
    "workflow.add_edge(\"assemble_draft\", END)\n",
    "\n",
    "# C. Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# ==========================================\n",
    "# 3. FINAL EXECUTION\n",
    "# ==========================================\n",
    "print(\"\\nðŸš€ STARTING FINAL TRUCV PIPELINE...\")\n",
    "\n",
    "final_state = app.invoke({\n",
    "    \"raw_text\": normalized_text,\n",
    "    \"retry_counts\": {}, \n",
    "    \"errors\": []\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ PIPELINE FINISHED.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DISPLAY RESULT\n",
    "# ==========================================\n",
    "import json\n",
    "\n",
    "if final_state.get(\"trucv_draft\"):\n",
    "    print(\"\\nâœ… FINAL JSON OUTPUT (Ready for MongoDB):\")\n",
    "    print(json.dumps(final_state[\"trucv_draft\"], indent=2))\n",
    "    \n",
    "    # Save to file for inspection\n",
    "    with open(\"trucv_draft.json\", \"w\") as f:\n",
    "        json.dump(final_state[\"trucv_draft\"], f, indent=2)\n",
    "    print(\"\\nðŸ’¾ Saved to 'trucv_draft.json'\")\n",
    "else:\n",
    "    print(\"\\nâŒ Errors prevented draft creation:\")\n",
    "    print(final_state[\"errors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1ecb2",
   "metadata": {},
   "source": [
    "## CELL 14 â€” Final Response Object & Schema Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ FINAL API RESPONSE PAYLOAD:\n",
      "{\n",
      "  \"status\": \"draft_created\",\n",
      "  \"trucvDraft\": {\n",
      "    \"userId\": \"user_12345_mock_id\",\n",
      "    \"title\": \"Uploaded Resume Draft\",\n",
      "    \"personal\": {\n",
      "      \"fullName\": \"Ganesh Agrahari\",\n",
      "      \"email\": \"ganeshagrahari08@gmail.com\",\n",
      "      \"phone\": \"+91 9044232872\",\n",
      "      \"city\": \"Lucknow, India\",\n",
      "      \"linkedin\": \"\",\n",
      "      \"github\": \"\",\n",
      "      \"summary\": \"\",\n",
      "      \"imgUrl\": \"\"\n",
      "    },\n",
      "    \"educations\": [\n",
      "      {\n",
      "        \"id\": \"e6f86a6d-8b55-4a2f-ad5a-43e0be5e0e29\",\n",
      "        \"eduDocId\": \"f9af1c3c-dced-4d26-ac3f-5cf73b32a945\",\n",
      "        \"level\": \"Undergraduate\",\n",
      "        \"boardNameOrDegree\": \"BCA Data Science & Artificial Intelligence\",\n",
      "        \"institutionName\": \"BBD University\",\n",
      "        \"gpa\": \"8\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"2023-08\",\n",
      "          \"to\": \"2026-09\"\n",
      "        },\n",
      "        \"selfAttested\": true,\n",
      "        \"docUri\": null,\n",
      "        \"issuerEmailId\": \"\",\n",
      "        \"isEmailSend\": false,\n",
      "        \"verified\": false,\n",
      "        \"status\": \"pending\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"3a4efcf9-201d-48da-ba11-dbea5fd04c3d\",\n",
      "        \"eduDocId\": \"aa321d64-cfa0-4f6b-ae50-23fd3a0c7a94\",\n",
      "        \"level\": \"Grade 12\",\n",
      "        \"boardNameOrDegree\": \"Intermediate(PCM)\",\n",
      "        \"institutionName\": \"SVM Inter College Ntpc\",\n",
      "        \"gpa\": \"82%\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"2022\",\n",
      "          \"to\": \"2022\"\n",
      "        },\n",
      "        \"selfAttested\": true,\n",
      "        \"docUri\": null,\n",
      "        \"issuerEmailId\": \"\",\n",
      "        \"isEmailSend\": false,\n",
      "        \"verified\": false,\n",
      "        \"status\": \"pending\"\n",
      "      }\n",
      "    ],\n",
      "    \"experiences\": [\n",
      "      {\n",
      "        \"id\": \"c2f2b750-0c94-472f-ae0d-ebf9aabc5adc\",\n",
      "        \"companyName\": \"Edubuk\",\n",
      "        \"jobRole\": \"AI Engineer Intern\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"2025-08\",\n",
      "          \"to\": \"Present\"\n",
      "        },\n",
      "        \"skills\": \"Azure, OpenAI, Elasticsearch\",\n",
      "        \"description\": \"- Built an AI-powered JD\\u2013CV matching system combining vector similarity for accuracy and LLM-based analysis for contextual understanding, enabling high-precision candidate\\u2013job matching at scale.\\n- Designed and deployed a serverless architecture on Azure using Azure Functions, including migration from an earlier AWS Lambda\\u2013based serverless setup, ensuring improved scalability and operational consistency.\\n- Implemented a Retrieval-Augmented pipeline using OpenAI text-embedding-3-large for semantic embeddings and GPT-4 for deep context evaluation between job descriptions and resumes.\\n- Migrated the search infrastructure from AWS OpenSearch to self-hosted Elasticsearch on Azure Virtual Machines, improving system control, flexibility, and long-term cost efficiency.\\n- Engineered backend services for resume ingestion, JD processing, and matching orchestration, delivering fast, reliable responses suitable for production-grade hiring workflows.\",\n",
      "        \"selfAttested\": true,\n",
      "        \"isEmailSend\": false,\n",
      "        \"docUri\": null,\n",
      "        \"issuerEmailId\": \"\",\n",
      "        \"verified\": false,\n",
      "        \"status\": \"pending\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"e127041c-6dfd-4622-91e5-c2424ff6f847\",\n",
      "        \"companyName\": \"QTechSolutions\",\n",
      "        \"jobRole\": \"AI/GenAi Intern\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"2023-03\",\n",
      "          \"to\": \"2025-08\"\n",
      "        },\n",
      "        \"skills\": \"NLP, machine learning\",\n",
      "        \"description\": \"- Working on a real-world healthcare site to develop and integrate an AI-powered chatbot for doctor consultations, medicine delivery, and prescription recommendations.\\n- Implementing NLP and machine learning techniques to enhance chatbot accuracy, ensuring seamless and efficient user interactions in a scalable AI-driven system.\\n- Leveraging Retrieval-Augmented Generation (RAG) to provide dynamic, context-aware responses by combining LLM capabilities with real-time retrieval from healthcare databases.\",\n",
      "        \"selfAttested\": true,\n",
      "        \"isEmailSend\": false,\n",
      "        \"docUri\": null,\n",
      "        \"issuerEmailId\": \"\",\n",
      "        \"verified\": false,\n",
      "        \"status\": \"pending\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"49e58069-63ec-48a5-b042-2fc1f86637dd\",\n",
      "        \"companyName\": \"Unified Mentor\",\n",
      "        \"jobRole\": \"Data Science Intern\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"2024-11\",\n",
      "          \"to\": \"2025-01\"\n",
      "        },\n",
      "        \"skills\": \"NLP, sentiment analysis, topic modeling\",\n",
      "        \"description\": \"- Implemented a text classification pipeline, improving document categorization accuracy by 20%\\n- Developed an NLP-based climate change analysis system using sentiment analysis and topic modeling to extract insights from global news articles.\",\n",
      "        \"selfAttested\": true,\n",
      "        \"isEmailSend\": false,\n",
      "        \"docUri\": null,\n",
      "        \"issuerEmailId\": \"\",\n",
      "        \"verified\": false,\n",
      "        \"status\": \"pending\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills\": [\n",
      "      {\n",
      "        \"id\": \"77d1fb8a-4c24-4826-b364-453ce1e436eb\",\n",
      "        \"skillName\": \"Python\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"cb580814-b859-48cb-a465-1a3f0282f15a\",\n",
      "        \"skillName\": \"JavaScript\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"c8275362-8925-4452-975c-45096d7913c3\",\n",
      "        \"skillName\": \"Async Programming\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"5d6f3946-5782-4648-8c42-6140033938bb\",\n",
      "        \"skillName\": \"REST API Development\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"74a66a7f-0870-4555-b40b-be137a2d6c58\",\n",
      "        \"skillName\": \"Machine Learning\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"bf8a486a-af2b-4b98-a4e6-eff40fb766eb\",\n",
      "        \"skillName\": \"Deep Learning\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"d2b25d3d-c3fa-43b2-a87e-bb1bb2f80a5a\",\n",
      "        \"skillName\": \"NLP\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"f0e1f248-8251-447e-aef0-127effd6dde8\",\n",
      "        \"skillName\": \"LLMs\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"ee2c1e57-8cdd-4bf1-9ca3-ea1d444bba31\",\n",
      "        \"skillName\": \"RAG\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"e1f14ee2-1ffb-4f6d-917c-04027cd7a33f\",\n",
      "        \"skillName\": \"Vector Search (Elasticsearch)\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"c763ef73-b7a7-43cc-a1cd-39eb4b6e3b80\",\n",
      "        \"skillName\": \"Multi-Vector Embeddings (3072-dim)\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"88a5efad-962b-4809-8e42-2d72a7a8620c\",\n",
      "        \"skillName\": \"Hybrid Scoring (GPT-4 + vector similarity)\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"92cfebf7-563d-401c-8271-4516f6309a6d\",\n",
      "        \"skillName\": \"Prompt Engineering\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"4727aa21-ba26-4e33-8fac-dcd0441ddfcb\",\n",
      "        \"skillName\": \"PyTorch\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"ea4218f7-7fa0-4280-80a3-7b667e7eed7a\",\n",
      "        \"skillName\": \"LangChain\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"3e716bc8-ce04-47ac-9853-908e8eb0d78c\",\n",
      "        \"skillName\": \"LangGraph\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"9e2f6522-3b6f-4b0a-be0e-1b158825dc68\",\n",
      "        \"skillName\": \"Scikit-learn\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"cf2d0491-36f7-43fa-acd6-e874a064ce6c\",\n",
      "        \"skillName\": \"OpenCV\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"fcbdf772-fe55-4913-a7b4-8f2bd9112419\",\n",
      "        \"skillName\": \"N8N\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"993d8670-6cf9-4d3d-a118-55467fa1cb46\",\n",
      "        \"skillName\": \"Azure\",\n",
      "        \"level\": \"Expert\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"37fdec19-043d-47aa-afa5-e6aee2f37f84\",\n",
      "        \"skillName\": \"AWS\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"f69dcb42-dc61-4e03-82a0-d8844476503e\",\n",
      "        \"skillName\": \"Microservices\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"5cf7cb8d-1d84-4b77-8f5e-aa5bb23b2f0d\",\n",
      "        \"skillName\": \"Serverless Architecture\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"b13cc52f-6ba8-456f-aafe-80079a14fc8c\",\n",
      "        \"skillName\": \"Async Pipelines\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"0ec65673-40d6-4ea2-a2ef-1047a80d8249\",\n",
      "        \"skillName\": \"CI/CD\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"88c1c844-6f0f-4f88-9500-4a493bf86101\",\n",
      "        \"skillName\": \"Git\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"3742d635-4fdc-4d47-a5d1-a85a9b3bcabe\",\n",
      "        \"skillName\": \"GitHub\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"fa9c7b93-48af-49b2-834a-a3df97d93101\",\n",
      "        \"skillName\": \"Data Processing\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"cc4cc245-fe2f-4639-bfed-1493eeb52766\",\n",
      "        \"skillName\": \"ETL\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"6ea574c6-5a36-4975-84df-4734105e696d\",\n",
      "        \"skillName\": \"Feature Engineering\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"c78bd461-a7e5-45b3-99a2-42f9100f428b\",\n",
      "        \"skillName\": \"Power BI\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"4168805c-a2d6-4c6b-b3ad-657fdd7703b7\",\n",
      "        \"skillName\": \"Jupyter Notebook\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"6bc30908-eed2-48b6-9d93-d9d807ee628c\",\n",
      "        \"skillName\": \"Matplotlib\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"b78f9a2d-9c8e-4094-8455-c453e3f45180\",\n",
      "        \"skillName\": \"Seaborn\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"01fbb22e-28ad-44d6-afaa-c9faedcfe964\",\n",
      "        \"skillName\": \"SQL\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"12aa54f1-9438-4a2e-9466-835e24c0cf24\",\n",
      "        \"skillName\": \"Elasticsearch (vector + keyword fields)\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"5608f924-3b96-4523-9a5b-67f2140847af\",\n",
      "        \"skillName\": \"AWS S3\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"22e46920-f8e8-4b30-8f1d-c27d51a8ab37\",\n",
      "        \"skillName\": \"Azure Blob Storage\",\n",
      "        \"level\": \"Intermediate\",\n",
      "        \"selfAttested\": true,\n",
      "        \"endoresBy\": \"\",\n",
      "        \"endoresThrough\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    \"projects\": [\n",
      "      {\n",
      "        \"id\": \"a2bb749b-bdd1-450d-9231-cd23a0e10bc4\",\n",
      "        \"projectName\": \"AI Recruitment System\",\n",
      "        \"projectUrl\": \"https://edubuk.com/trujobs\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"\",\n",
      "          \"to\": \"\"\n",
      "        },\n",
      "        \"skills\": \"AI, OpenAI, Elasticsearch\",\n",
      "        \"description\": \"Designed an AI-driven JD\\u2013CV matching workflow that evaluates both semantic similarity and contextual relevance, improving hiring signal quality beyond keyword-based matching.\",\n",
      "        \"selfAttested\": true\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"1942a296-6c6d-48a0-8de2-1c494670a05b\",\n",
      "        \"projectName\": \"Face Recognition Attendance System\",\n",
      "        \"projectUrl\": \"\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"\",\n",
      "          \"to\": \"\"\n",
      "        },\n",
      "        \"skills\": \"Python, OpenCV, CNN\",\n",
      "        \"description\": \"Designed and deployed a real-time face recognition attendance system using Python, OpenCV, and CNN, achieving 95%+ accuracy and reducing manual attendance tracking efforts by over 40%.\",\n",
      "        \"selfAttested\": true\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"71aaec6f-7bb2-4d4d-b97e-1e8f2d513cba\",\n",
      "        \"projectName\": \"Next.js Portfolio site\",\n",
      "        \"projectUrl\": \"https://vercel.com/nextjs-portfolio\",\n",
      "        \"duration\": {\n",
      "          \"from\": \"\",\n",
      "          \"to\": \"\"\n",
      "        },\n",
      "        \"skills\": \"Next.js, Vercel\",\n",
      "        \"description\": \"Interactive Portfolio Website - Built using Next.js and deployed on Vercel, featuring a responsive design and an AI-powered chatbot for seamless user interaction.\",\n",
      "        \"selfAttested\": true\n",
      "      }\n",
      "    ],\n",
      "    \"awards\": []\n",
      "  },\n",
      "  \"confidenceMeta\": {\n",
      "    \"personal\": 0.96,\n",
      "    \"education\": 1.0,\n",
      "    \"experience\": 1.0\n",
      "  },\n",
      "  \"warnings\": [\n",
      "    \"INFO: LinkedIn profile not found.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ðŸ” RUNNING SCHEMA COMPATIBILITY AUDIT...\n",
      "âœ… SUCCESS: Generated JSON is 100% compatible with newCv.model.ts\n",
      "   Ready for Database Insertion.\n"
     ]
    }
   ],
   "source": [
    "# ðŸŸ¦ CELL 14 â€” Final Response Object & Schema Audit\n",
    "\n",
    "import json\n",
    "\n",
    "# 1. Construct the Final API Response\n",
    "# This mimics the JSON payload your Azure Function will return to the UI\n",
    "api_response = {\n",
    "    \"status\": \"draft_created\" if final_state.get(\"trucv_draft\") else \"failed\",\n",
    "    \"trucvDraft\": final_state.get(\"trucv_draft\", {}),\n",
    "    \"confidenceMeta\": final_state.get(\"confidence_map\", {}),\n",
    "    \"warnings\": final_state.get(\"errors\", [])\n",
    "}\n",
    "\n",
    "# 2. Display the Payload\n",
    "print(\"\\nðŸ“¦ FINAL API RESPONSE PAYLOAD:\")\n",
    "print(json.dumps(api_response, indent=2))\n",
    "\n",
    "# ====================================================\n",
    "# 3. AUTOMATED COMPATIBILITY CHECK (The \"Sanity Check\")\n",
    "# ====================================================\n",
    "print(\"\\nðŸ” RUNNING SCHEMA COMPATIBILITY AUDIT...\")\n",
    "\n",
    "draft = api_response[\"trucvDraft\"]\n",
    "issues = []\n",
    "\n",
    "# CHECK 1: Root Keys\n",
    "required_root_keys = [\"personal\", \"educations\", \"experiences\", \"skills\", \"projects\", \"awards\"]\n",
    "for key in required_root_keys:\n",
    "    if key not in draft:\n",
    "        issues.append(f\"âŒ Root key missing: '{key}'\")\n",
    "\n",
    "# CHECK 2: Personal Section Details\n",
    "if \"personal\" in draft:\n",
    "    p = draft[\"personal\"]\n",
    "    # Check a few critical fields required by Mongoose\n",
    "    if \"fullName\" not in p: issues.append(\"âŒ Personal: 'fullName' is missing\")\n",
    "    if \"email\" not in p: issues.append(\"âŒ Personal: 'email' is missing\")\n",
    "\n",
    "# CHECK 3: Education Structure (Array + IDs)\n",
    "if draft.get(\"educations\"):\n",
    "    edu = draft[\"educations\"][0]\n",
    "    if \"id\" not in edu: issues.append(\"âŒ Education: 'id' (UUID) is missing\")\n",
    "    if \"eduDocId\" not in edu: issues.append(\"âŒ Education: 'eduDocId' is missing (Backend requires this)\")\n",
    "    if \"status\" not in edu: issues.append(\"âŒ Education: 'status' is missing\")\n",
    "    if \"duration\" not in edu or \"from\" not in edu[\"duration\"]: \n",
    "        issues.append(\"âŒ Education: 'duration' structure is incorrect\")\n",
    "\n",
    "# CHECK 4: Experience Structure\n",
    "if draft.get(\"experiences\"):\n",
    "    exp = draft[\"experiences\"][0]\n",
    "    if \"companyName\" not in exp: issues.append(\"âŒ Experience: 'companyName' missing\")\n",
    "    if \"skills\" not in exp: issues.append(\"âŒ Experience: 'skills' field missing\")\n",
    "    # Verify strict status enum\n",
    "    if exp.get(\"status\") not in [\"pending\", \"verified\"]:\n",
    "         issues.append(f\"âŒ Experience: Invalid status '{exp.get('status')}' (Must be 'pending')\")\n",
    "\n",
    "# VERDICT\n",
    "if not issues:\n",
    "    print(\"âœ… SUCCESS: Generated JSON is 100% compatible with newCv.model.ts\")\n",
    "    print(\"   Ready for Database Insertion.\")\n",
    "else:\n",
    "    print(\"âš ï¸ COMPATIBILITY ISSUES DETECTED:\")\n",
    "    for issue in issues:\n",
    "        print(issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc1be4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›¡ï¸ RUNNING TRUCV COMPLIANCE CHECKS...\n",
      "\n",
      "âœ… TRUST CHECK PASSED: All data is correctly marked 'unverified'.\n",
      "âœ… DATA CHECK PASSED: Draft contains contact info + content.\n",
      "âœ… TRANSPARENCY PASSED: Confidence scores generated for ['personal', 'education', 'experience'].\n",
      "\n",
      "ðŸ“Š FINAL SUMMARY REPORT\n",
      "   - Status:      draft_created\n",
      "   - ID:          user_12345_mock_id\n",
      "   - Warnings:    1 detected\n",
      "     First Warning: 'INFO: LinkedIn profile not found.'\n",
      "\n",
      "ðŸŽ¯ FINAL VERDICT:\n",
      "   ðŸš€ PROTOTYPE SUCCESSFUL. READY FOR AZURE MIGRATION.\n"
     ]
    }
   ],
   "source": [
    "# ðŸŸ¦ CELL 15 â€” Manual Sanity Check & Final Verdict\n",
    "# This cell proves that the output isn't just \"valid JSON\", but \"TruCV Compliant\".\n",
    "\n",
    "print(\"ðŸ›¡ï¸ RUNNING TRUCV COMPLIANCE CHECKS...\\n\")\n",
    "\n",
    "draft = api_response[\"trucvDraft\"]\n",
    "meta = api_response[\"confidenceMeta\"]\n",
    "warnings = api_response[\"warnings\"]\n",
    "\n",
    "# 1. VERIFY \"TRUST NO ONE\" POLICY\n",
    "# Rule: AI must never mark data as 'verified'.\n",
    "verified_flags = []\n",
    "if \"educations\" in draft:\n",
    "    verified_flags.extend([e[\"verified\"] for e in draft[\"educations\"]])\n",
    "if \"experiences\" in draft:\n",
    "    verified_flags.extend([e[\"verified\"] for e in draft[\"experiences\"]])\n",
    "\n",
    "if any(verified_flags):\n",
    "    print(\"âŒ SECURITY FAILURE: AI marked some data as 'verified'. This is dangerous.\")\n",
    "else:\n",
    "    print(\"âœ… TRUST CHECK PASSED: All data is correctly marked 'unverified'.\")\n",
    "\n",
    "# 2. VERIFY DATA INTEGRITY\n",
    "# Rule: We need at least one contact method and one content section.\n",
    "has_contact = bool(draft[\"personal\"].get(\"email\") or draft[\"personal\"].get(\"phone\"))\n",
    "has_content = bool(draft[\"educations\"] or draft[\"experiences\"] or draft[\"projects\"])\n",
    "\n",
    "if has_contact and has_content:\n",
    "    print(\"âœ… DATA CHECK PASSED: Draft contains contact info + content.\")\n",
    "else:\n",
    "    print(\"âš ï¸ DATA WARNING: Result looks empty. Check text extraction.\")\n",
    "\n",
    "# 3. VERIFY CONFIDENCE TRANSPARENCY\n",
    "# Rule: We must expose how confident we are.\n",
    "if meta and len(meta) > 0:\n",
    "    print(f\"âœ… TRANSPARENCY PASSED: Confidence scores generated for {list(meta.keys())}.\")\n",
    "else:\n",
    "    print(\"âŒ FAILURE: No confidence metadata found.\")\n",
    "\n",
    "# 4. FINAL HUMAN REPORT\n",
    "print(f\"\\nðŸ“Š FINAL SUMMARY REPORT\")\n",
    "print(f\"   - Status:      {api_response['status']}\")\n",
    "print(f\"   - ID:          {draft.get('userId', 'N/A')}\")\n",
    "print(f\"   - Warnings:    {len(warnings)} detected\")\n",
    "if len(warnings) > 0:\n",
    "    print(f\"     First Warning: '{warnings[0]}'\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ FINAL VERDICT:\")\n",
    "if not any(verified_flags) and has_contact and has_content:\n",
    "    print(\"   ðŸš€ PROTOTYPE SUCCESSFUL. READY FOR AZURE MIGRATION.\")\n",
    "else:\n",
    "    print(\"   ðŸ›‘ PROTOTYPE NEEDS REFINEMENT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
